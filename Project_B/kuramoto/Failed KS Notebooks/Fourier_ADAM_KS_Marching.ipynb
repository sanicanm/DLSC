{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ey2CYxi6aWQ",
    "outputId": "bdaeda10-2fbb-483b-81ab-bcd167dda60c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2ace3a8a870>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.utils as nn_utils\n",
    "import math\n",
    "import scipy\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\" )\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "torch.manual_seed(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rsY1JtlIQ-oq"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import os\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dimension,\n",
    "                 output_dimension,\n",
    "                 n_hidden_layers,\n",
    "                 neurons,\n",
    "                 regularization_param,\n",
    "                 regularization_exp,\n",
    "                 retrain_seed,\n",
    "                 L=1.0,\n",
    "                 M=1):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        # Number of input dimensions n\n",
    "        self.input_dimension = input_dimension\n",
    "        # Number of output dimensions m\n",
    "        self.output_dimension = output_dimension\n",
    "        # Number of neurons per layer\n",
    "        self.neurons = neurons\n",
    "        # Number of hidden layers\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        # Activation function\n",
    "        self.activation = nn.Tanh()\n",
    "        self.regularization_param = regularization_param\n",
    "        # Regularization exponent\n",
    "        self.regularization_exp = regularization_exp\n",
    "        # Random seed for weight initialization\n",
    "\n",
    "        self.input_layer = nn.Linear(self.input_dimension, self.neurons)\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(self.neurons, self.neurons)\n",
    "            for _ in range(n_hidden_layers - 1)\n",
    "        ])\n",
    "        self.output_layer = nn.Linear(self.neurons, self.output_dimension)\n",
    "        self.retrain_seed = retrain_seed\n",
    "        # Random Seed for weight initialization\n",
    "        self.init_xavier()\n",
    "        self.L = L\n",
    "        self.M = M\n",
    "\n",
    "    def input_encoding(self, t, x):\n",
    "        w = 2.0 * math.pi / self.L\n",
    "        k = torch.arange(1, self.M + 1).to(device)\n",
    "        out = torch.hstack([\n",
    "            t.unsqueeze(1).to(device),\n",
    "            torch.ones((t.shape[0],1)).to(device),\n",
    "            torch.cos(w* torch.einsum('i,j->ji',k , x).to(device)),\n",
    "            torch.sin(w* torch.einsum('i,j->ji',k , x).to(device))\n",
    "        ])\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The forward function performs the set of affine and non-linear\n",
    "        # transformations defining the network (see equation above)\n",
    "        x = x.to(device)\n",
    "        x_encoded = self.input_encoding(x[:,0], x[:,1])\n",
    "        x_encoded = self.activation(self.input_layer(x_encoded))\n",
    "        for k, l in enumerate(self.hidden_layers):\n",
    "            x_encoded = self.activation(l(x_encoded))\n",
    "        return self.output_layer(x_encoded)\n",
    "\n",
    "    def init_xavier(self):\n",
    "        torch.manual_seed(self.retrain_seed)\n",
    "\n",
    "        def init_weights(m):\n",
    "            if type(\n",
    "                    m\n",
    "            ) == nn.Linear and m.weight.requires_grad and m.bias.requires_grad:\n",
    "                g = nn.init.calculate_gain('tanh')\n",
    "                torch.nn.init.xavier_uniform_(m.weight, gain=g)\n",
    "                # torch.nn.init.xavier_normal_(m.weight, gain=g)\n",
    "                m.bias.data.fill_(0)\n",
    "\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def regularization(self):\n",
    "        reg_loss = 0\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                reg_loss = reg_loss + torch.norm(param, self.regularization_exp)\n",
    "        return self.regularization_param * reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lfJFeUC86aWS"
   },
   "outputs": [],
   "source": [
    "class Pinns_Kuramoto:\n",
    "    def __init__(self, n_int_t_, n_int_x_, n_sb_, n_tb_, t_0_, t_1_, init_cond_, L=1.0, M=1):\n",
    "        self.n_int_t = n_int_t_\n",
    "        self.n_int_x = n_int_x_\n",
    "        self.n_sb = n_sb_\n",
    "        self.n_tb = n_tb_\n",
    "        self.t0 = t_0_\n",
    "        self.t1 = t_1_\n",
    "        self.lambda_f = 50\n",
    "        self.init_cond = init_cond_\n",
    "\n",
    "        #Set constants\n",
    "        self.alpha = 5\n",
    "        self.beta = 0.5\n",
    "        self.gamma = 0.005\n",
    "\n",
    "        # Extrema of the solution domain \n",
    "        self.domain_extrema = torch.tensor([[self.t0,  self.t1],  [-1, 1]]) \n",
    "\n",
    "        # Number of space dimensions\n",
    "        self.space_dimensions = 1\n",
    "\n",
    "\n",
    "        # NN to Approximate Solution\n",
    "        self.approximate_solution_flame = NeuralNet(input_dimension=2*M+2, output_dimension=1,\n",
    "                                              n_hidden_layers=5,\n",
    "                                              neurons=120,\n",
    "                                              regularization_param=0.01,\n",
    "                                              regularization_exp=2.,\n",
    "                                              retrain_seed=42,L=2.0, M=1).to(device)\n",
    "      # Generator of Sobol sequences\n",
    "        self.soboleng = torch.quasirandom.SobolEngine(dimension=self.domain_extrema.shape[0], scramble = True)\n",
    "        self.soboleng_int = torch.quasirandom.SobolEngine(dimension=1, scramble = True)\n",
    "\n",
    "      # Training Sets S_tb, S_sb, S_int as Torch Dataloader\n",
    "        self.training_set_tb, self.training_set_sb, self.training_set_int, = self.assemble_datasets()\n",
    "\n",
    "    ################################################################################################\n",
    "\n",
    "    def convert(self, tens):\n",
    "        assert (tens.shape[1] == self.domain_extrema.shape[0])\n",
    "        return tens * (self.domain_extrema[:, 1] - self.domain_extrema[:, 0]) + self.domain_extrema[:, 0]\n",
    "\n",
    "    def convert_time(self, tens):\n",
    "        #assert (tens.shape[1] == self.domain_extrema.shape[0])\n",
    "        return tens * (self.domain_extrema[0, 1] - self.domain_extrema[0, 0]) + self.domain_extrema[0, 0]\n",
    "\n",
    "    def convert_space(self, tens):\n",
    "        #assert (tens.shape[] == self.domain_extrema.shape[0])\n",
    "        return tens * (self.domain_extrema[1, 1] - self.domain_extrema[1, 0]) + self.domain_extrema[1, 0]\n",
    "\n",
    "    def initial_conditions(self, x):\n",
    "        if self.init_cond is None:\n",
    "            return - torch.sin(math.pi * x)\n",
    "        else:\n",
    "            return self.init_cond.detach()\n",
    "\n",
    "    def create_left_sb_mask(self, inp_sb):\n",
    "        left_sb_mask = (inp_sb[:, 1] == -1)\n",
    "        return left_sb_mask\n",
    "\n",
    "    def create_right_sb_mask(self, inp_sb):\n",
    "        right_sb_mask = (inp_sb[:, 1] == 1)\n",
    "        return right_sb_mask\n",
    "\n",
    "    ################################################################################################\n",
    "    # Function returning the input-output tensor required to assemble the training set S_tb corresponding to the temporal boundary\n",
    "    def add_temporal_boundary_points(self):\n",
    "        t0 = self.domain_extrema[0, 0]\n",
    "        x0 = self.domain_extrema[1, 0]\n",
    "        xL = self.domain_extrema[1, 1]\n",
    "        \n",
    "        input_tb_x = torch.linspace(x0, xL, self.n_tb)\n",
    "        input_tb_t = torch.full_like(input_tb_x, self.t0)\n",
    "        input_tb = torch.cat((input_tb_t.unsqueeze(1),input_tb_x.unsqueeze(1)), dim=1)\n",
    "        output_tb = self.initial_conditions(input_tb[:, 1]).reshape(-1, 1)\n",
    "        \n",
    "        return input_tb, output_tb\n",
    "\n",
    "    # Function returning the input-output tensor required to assemble the training set S_sb corresponding to the spatial boundary\n",
    "    def add_spatial_boundary_points(self):\n",
    "        x0 = self.domain_extrema[1, 0]\n",
    "        xL = self.domain_extrema[1, 1]\n",
    "\n",
    "        input_sb = self.convert(self.soboleng.draw(self.n_sb))\n",
    "\n",
    "        input_sb_0 = torch.clone(input_sb)\n",
    "        input_sb_0[:, 1] = torch.full(input_sb_0[:, 1].shape, x0)\n",
    "\n",
    "        input_sb_L = torch.clone(input_sb)\n",
    "        input_sb_L[:, 1] = torch.full(input_sb_L[:, 1].shape, xL)\n",
    "\n",
    "        output_sb_0 = torch.zeros((input_sb.shape[0], 1))\n",
    "        output_sb_L = torch.zeros((input_sb.shape[0], 1))\n",
    "\n",
    "        return torch.cat([input_sb_0, input_sb_L], 0), torch.cat([output_sb_0, output_sb_L], 0)\n",
    "\n",
    "    #  Function returning the input-output tensor required to assemble the training set S_int corresponding to the interior domain where the PDE is enforced\n",
    "    def add_interior_points(self):\n",
    "        \"\"\"\n",
    "        t0 = self.domain_extrema[0, 0]\n",
    "        t1 = self.domain_extrema[0, 1]\n",
    "        x0 = self.domain_extrema[1, 0]\n",
    "        xL = self.domain_extrema[1, 1]\n",
    "        \n",
    "        inpt = torch.linspace(t0 + self.epsilon, t1 - self.epsilon, self.n_int_t)\n",
    "        inpx = torch.linspace(x0 + self.epsilon, xL - self.epsilon, self.n_int_x)\n",
    "        tt, xx = torch.meshgrid(inpt, inpx)\n",
    "\n",
    "        input_int = torch.cat((tt.flatten().unsqueeze(1), xx.flatten().unsqueeze(1)), dim=1)\n",
    "        output_int = torch.zeros((input_int.shape[0], 1))\n",
    "        \"\"\"\n",
    "        \n",
    "        input_time = self.convert(self.soboleng.draw(self.n_int_t))\n",
    "        inpt = input_time[:, 0].unique()\n",
    "        inpx = self.convert_space(self.soboleng_int.draw(self.n_int_x)).squeeze(1)\n",
    "        tt, xx = torch.meshgrid(inpt, inpx)\n",
    "\n",
    "        input_int = torch.cat((tt.flatten().unsqueeze(1), xx.flatten().unsqueeze(1)), dim=1)\n",
    "        output_int = torch.zeros((input_int.shape[0], 1))\n",
    "        \n",
    "        return input_int, output_int\n",
    "\n",
    "\n",
    "    # Function returning the training sets S_sb, S_tb, S_int as dataloader\n",
    "    def assemble_datasets(self):\n",
    "        input_tb, output_tb = self.add_temporal_boundary_points()  # S_tb\n",
    "        input_sb, output_sb = self.add_spatial_boundary_points()  # S_sb\n",
    "        input_int, output_int = self.add_interior_points()  # S_int\n",
    "\n",
    "        input_tb, output_tb = input_tb.to(device), output_tb.to(device)\n",
    "        input_sb, output_sb = input_sb.to(device), output_sb.to(device)\n",
    "        input_int, output_int = input_int.to(device), output_int.to(device)\n",
    "\n",
    "        training_set_tb = DataLoader(torch.utils.data.TensorDataset(input_tb, output_tb), batch_size=self.n_tb, shuffle=False)\n",
    "        training_set_sb = DataLoader(torch.utils.data.TensorDataset(input_sb, output_sb), batch_size=2 * self.space_dimensions * self.n_sb, shuffle=False) #2 * self.space_dimensions * self.n_sb\n",
    "        training_set_int = DataLoader(torch.utils.data.TensorDataset(input_int, output_int), batch_size=self.n_int_t * self.n_int_x, shuffle=False)\n",
    "\n",
    "        return training_set_tb, training_set_sb, training_set_int\n",
    "\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "    #   Compute Temporal Boundary Residuals\n",
    "    def compute_temporal_boundary_residual(self, input_tb, output_tb):\n",
    "        assert(torch.all(input_tb[:,0] == self.t0))\n",
    "        u = self.approximate_solution_flame(input_tb).reshape(-1,)\n",
    "        temp_tb_train = output_tb.reshape(-1,)\n",
    "\n",
    "        assert(u.shape == temp_tb_train.shape)\n",
    "        residual_tb = temp_tb_train - u\n",
    "\n",
    "        return residual_tb.reshape(-1,)\n",
    "    \"\"\"\n",
    "    #   Compute Spatial Boundary Residuals\n",
    "    def compute_spatial_boundary_residual(self, input_sb, output_sb):\n",
    "        u = self.approximate_solution_flame(input_sb).reshape(-1,)\n",
    "\n",
    "        mask_L = self.create_left_sb_mask(input_sb)\n",
    "        mask_R = self.create_right_sb_mask(input_sb)\n",
    "        assert(u[mask_L].shape == u[mask_R].shape)\n",
    "\n",
    "        residual_sb = u[mask_L] - u[mask_R]\n",
    "\n",
    "        return residual_sb.reshape(-1,)\n",
    "    \"\"\"\n",
    "    #   Compute Interior Residuals\n",
    "    def compute_interior_residual(self, input_int):\n",
    "        input_int.requires_grad = True\n",
    "        u = self.approximate_solution_flame(input_int).reshape(-1,)\n",
    "\n",
    "        grad_u = torch.autograd.grad(u.sum(), input_int, create_graph=True)[0]\n",
    "        grad_u_t = grad_u[:, 0]\n",
    "        grad_u_x = grad_u[:, 1]\n",
    "        grad_u_xx = torch.autograd.grad(grad_u_x.sum(), input_int, create_graph=True)[0][:, 1]\n",
    "        grad_u_xxx = torch.autograd.grad(grad_u_xx.sum(), input_int, create_graph=True)[0][:, 1]\n",
    "        grad_u_xxxx = torch.autograd.grad(grad_u_xxx.sum(), input_int, create_graph=True)[0][:, 1]\n",
    "\n",
    "        assert(u.shape == grad_u_t.shape and u.shape == grad_u_x.shape and u.shape == grad_u_xx.shape )\n",
    "\n",
    "        residual_int = grad_u_t + self.alpha*u*grad_u_x + self.beta*grad_u_xx + self.gamma*grad_u_xxxx\n",
    "        \n",
    "        return residual_int.reshape(-1,)\n",
    "        #return residual_int.reshape(self.n_int_x,self.n_int_t)\n",
    "\n",
    "\n",
    "    #   Function to compute the total loss (weighted sum of spatial boundary loss, temporal boundary loss and interior loss)\n",
    "    def compute_loss(self, inp_train_tb, T_train_tb, inp_train_sb, T_train_sb, inp_train_int, T_train_int, verbose=True):\n",
    "        \n",
    "        # Temporal Boundary Residuals\n",
    "        r_tb = self.compute_temporal_boundary_residual(inp_train_tb, T_train_tb)\n",
    "\n",
    "        # Interior Residuals\n",
    "        r_int = self.compute_interior_residual(inp_train_int)\n",
    "\n",
    "        loss_tb = torch.mean(abs(r_tb) ** 2)\n",
    "        loss_int = torch.mean(abs(r_int) ** 2)\n",
    "\n",
    "        loss = torch.log10(self.lambda_f * loss_tb  + loss_int)\n",
    "\n",
    "        return loss, torch.log10(loss_tb), torch.log10(loss_int)\n",
    "    \n",
    "    ################################################################################################\n",
    "    \n",
    "    \n",
    "    def fit(self, num_epochs, optimizer, verbose=True):\n",
    "        history = list()\n",
    "        # Loop over epochs\n",
    "        pbar = trange(num_epochs)\n",
    "        for epoch in pbar:\n",
    "            for j, ((inp_train_tb, u_train_tb), (inp_train_sb, u_train_sb), (inp_train_int, u_train_int)) in enumerate(zip(self.training_set_tb, self.training_set_sb, self.training_set_int)):\n",
    "                optimizer.zero_grad()\n",
    "                loss, loss_f, loss_i = self.compute_loss(inp_train_tb, u_train_tb, inp_train_sb, u_train_sb, inp_train_int, u_train_int, verbose=True)\n",
    "                loss.backward()\n",
    "                #nn_utils.clip_grad_norm_(self.approximate_solution_fluid.parameters(), max_norm=1.0)\n",
    "\n",
    "                history.append(loss.item())\n",
    "                optimizer.step()\n",
    "            \n",
    "            scheduler.step()\n",
    "            pbar.set_description(f\"LR = {round(optimizer.param_groups[0]['lr'], 6)}, Loss = {round(loss.item(),4)}, Function = {round(loss_f.item(),4)}, PDE = {round(loss_i.item(),4)}\")\n",
    "        \n",
    "        print('Final Loss: ', history[-1])\n",
    "\n",
    "        return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AES0I12C6aWT",
    "outputId": "c8c02017-1b36-4029-ceb7-de923ba40f30"
   },
   "outputs": [],
   "source": [
    "n_int_t = 64\n",
    "n_int_x = 128\n",
    "n_sb = 512\n",
    "n_tb = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "z12E1evS6aWU",
    "outputId": "cf042445-07f9-4ef3-e9a7-d9da9323920b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matth\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\functional.py:505: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3491.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "LR = 0.000314, Loss = 0.3967, Function = -2.4024, PDE = 0.3608:  97%|█████████████▌| 2915/3000 [14:00<00:24,  3.47it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m inputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconvert(inputs)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Train Time Interval\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m hist \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m                \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_ADAM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m                \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03mplt.figure(dpi=150)\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03mplt.grid(True, which=\"both\", ls=\":\")\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03mplt.legend()\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     39\u001b[0m x_init \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, n_tb)\n",
      "Cell \u001b[1;32mIn[4], line 211\u001b[0m, in \u001b[0;36mPinns_Kuramoto.fit\u001b[1;34m(self, num_epochs, optimizer, verbose)\u001b[0m\n\u001b[0;32m    209\u001b[0m pbar \u001b[38;5;241m=\u001b[39m trange(num_epochs)\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j, ((inp_train_tb, u_train_tb), (inp_train_sb, u_train_sb), (inp_train_int, u_train_int)) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_set_tb, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_set_sb, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_set_int)):\n\u001b[0;32m    212\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    213\u001b[0m         loss, loss_f, loss_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(inp_train_tb, u_train_tb, inp_train_sb, u_train_sb, inp_train_int, u_train_int, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 2000\n",
    "\n",
    "# Marching Steps\n",
    "N = 10\n",
    "init_cond = None\n",
    "\n",
    "# Create Array to Store Plots\n",
    "final_image = torch.tensor([]).to(device)\n",
    "final_inputs = torch.tensor([]).to(device)\n",
    "\n",
    "\n",
    "for i in range(N):\n",
    "    if i == 1:\n",
    "        n_epochs = 1\n",
    "    # Initialise Model for New Time Interval\n",
    "    t_0, t_1 = (1/N) * i, (1/N) * (i+1)    \n",
    "    model = Pinns_Kuramoto(n_int_t, n_int_x, n_sb, n_tb, t_0, t_1, init_cond)\n",
    "    \n",
    "    # Create New Optimizer\n",
    "    optimizer_ADAM = optim.Adam(model.approximate_solution_fluid.parameters(), lr=0.001, weight_decay=0)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer_ADAM, step_size=75, gamma=0.97)\n",
    "    \n",
    "    # Inputs for Plots\n",
    "    inputs = model.soboleng.draw(50000)\n",
    "    inputs = model.convert(inputs).to(device)\n",
    "    \n",
    "    # Train Time Interval\n",
    "    hist = model.fit(num_epochs=n_epochs,\n",
    "                    optimizer=optimizer_ADAM,\n",
    "                    verbose=True)\n",
    "    \"\"\"\n",
    "    plt.figure(dpi=150)\n",
    "    plt.grid(True, which=\"both\", ls=\":\")\n",
    "    plt.plot(np.arange(1, len(hist) + 1), hist, label=\"Train Loss\")\n",
    "    plt.xscale(\"log\")\n",
    "    plt.legend()\n",
    "    \"\"\"\n",
    "    \n",
    "    x_init = torch.linspace(-1, 1, n_tb)\n",
    "    t_init = torch.full_like(x_init, t_1)\n",
    "    inp_init = torch.cat((t_init.unsqueeze(1),x_init.unsqueeze(1)), dim=1).to(device)\n",
    "    init_cond = model.approximate_solution_flame(inp_init)\n",
    "\n",
    "    output_fluid = model.approximate_solution_flame(inputs.to(device))\n",
    "    final_inputs = torch.cat((final_inputs, inputs), dim = 0).to(device)\n",
    "    final_image = torch.cat((final_image, output_fluid), dim = 0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 791
    },
    "id": "EDL2yCdGQEYe",
    "outputId": "1c029518-0357-4e82-9913-85cbb6b2a023"
   },
   "outputs": [],
   "source": [
    "# Plot results with colour\n",
    "def plotting():\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(16, 8), dpi=150)\n",
    "    im1 = axs.scatter(final_inputs[:, 1].cpu().detach(), final_inputs[:, 0].cpu().detach(), c=final_image.cpu().detach(), cmap=\"jet\", s=2, vmin=-2.5, vmax=2.5)\n",
    "    axs.set_xlabel(\"x\")\n",
    "    axs.set_ylabel(\"t\")\n",
    "    plt.colorbar(im1, ax=axs)\n",
    "    axs.grid(True, which=\"both\", ls=\":\")\n",
    "    axs.set_title(\"Flame Front\")\n",
    "    plt.show()\n",
    "\n",
    "plotting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "submission = torch.cat((final_inputs, final_image), dim=1)\n",
    "submission = pd.DataFrame(submission.cpu().detach().numpy())\n",
    "\n",
    "# Save the DataFrame as a text file\n",
    "submission.to_csv(r\"C:\\Users\\matth\\Downloads\\submission_kuramoto.txt\", sep=',', index=False, header=['t', 'x', 'u'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

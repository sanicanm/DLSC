{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ey2CYxi6aWQ",
    "outputId": "a4b8f67a-e560-4cca-fece-b654a082e05d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e8d61d2670>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "torch.manual_seed(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rsY1JtlIQ-oq"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import os\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dimension, output_dimension, n_hidden_layers, neurons, regularization_param, regularization_exp, retrain_seed):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        # Number of input dimensions n\n",
    "        self.input_dimension = input_dimension\n",
    "        # Number of output dimensions m\n",
    "        self.output_dimension = output_dimension\n",
    "        # Number of neurons per layer\n",
    "        self.neurons = neurons\n",
    "        # Number of hidden layers\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        # Activation function\n",
    "        self.activation = nn.Tanh()\n",
    "        self.regularization_param = regularization_param\n",
    "        # Regularization exponent\n",
    "        self.regularization_exp = regularization_exp\n",
    "        # Random seed for weight initialization\n",
    "\n",
    "        self.input_layer = nn.Linear(self.input_dimension, self.neurons)\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(self.neurons, self.neurons) for _ in range(n_hidden_layers - 1)])\n",
    "        self.output_layer = nn.Linear(self.neurons, self.output_dimension)\n",
    "        self.retrain_seed = retrain_seed\n",
    "        # Random Seed for weight initialization\n",
    "        self.init_xavier()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The forward function performs the set of affine and non-linear transformations defining the network\n",
    "        # (see equation above)\n",
    "        x = self.activation(self.input_layer(x))\n",
    "        for k, l in enumerate(self.hidden_layers):\n",
    "            x = self.activation(l(x))\n",
    "        return self.output_layer(x)\n",
    "\n",
    "    def init_xavier(self):\n",
    "        torch.manual_seed(self.retrain_seed)\n",
    "\n",
    "        def init_weights(m):\n",
    "            if type(m) == nn.Linear and m.weight.requires_grad and m.bias.requires_grad:\n",
    "                g = nn.init.calculate_gain('tanh')\n",
    "                torch.nn.init.xavier_uniform_(m.weight, gain=g)\n",
    "                # torch.nn.init.xavier_normal_(m.weight, gain=g)\n",
    "                m.bias.data.fill_(0)\n",
    "\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def regularization(self):\n",
    "        reg_loss = 0\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                reg_loss = reg_loss + torch.norm(param, self.regularization_exp)\n",
    "        return self.regularization_param * reg_loss\n",
    "\n",
    "\n",
    "def fit(model, training_set, num_epochs, optimizer, p, verbose=True):\n",
    "    history = list()\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        if verbose: print(\"################################ \", epoch, \" ################################\")\n",
    "\n",
    "        running_loss = list([0])\n",
    "\n",
    "        # Loop over batches\n",
    "        for j, (x_train_, u_train_) in enumerate(training_set):\n",
    "            def closure():\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # forward + backward + optimize\n",
    "                u_pred_ = model(x_train_)\n",
    "                # Item 1. below\n",
    "                loss = torch.mean((u_pred_.reshape(-1, ) - u_train_.reshape(-1, )) ** p) + model.regularization()\n",
    "                # Item 2. below\n",
    "                loss.backward()\n",
    "                # Compute average training loss over batches for the current epoch\n",
    "                running_loss[0] += loss.item()\n",
    "                return loss\n",
    "\n",
    "            # Item 3. below\n",
    "            optimizer.step(closure=closure)\n",
    "\n",
    "        if verbose: print('Loss: ', (running_loss[0] / len(training_set)))\n",
    "        history.append(running_loss[0])\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lfJFeUC86aWS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Pinns:\n",
    "    def __init__(self, n_int_, n_sb_, n_tb_):\n",
    "        self.n_int = n_int_\n",
    "        self.n_sb = n_sb_\n",
    "        self.n_tb = n_tb_\n",
    "\n",
    "\n",
    "        #Set constants\n",
    "        self.alpha_f = 0.05\n",
    "        self.alpha_s = 0.08\n",
    "        self.h_f = 5\n",
    "        self.h_s = 6\n",
    "        self.T_hot = 4\n",
    "        self.T_0 = 1\n",
    "        self.U_f = 1\n",
    "\n",
    "\n",
    "        # Extrema of the solution domain (t,x) in [0,8]x[0,1]\n",
    "        self.domain_extrema = torch.tensor([[0, 1],  # Time dimension\n",
    "                                            [0, 1]])  # Space dimension\n",
    "\n",
    "        # Number of space dimensions\n",
    "        self.space_dimensions = 1\n",
    "\n",
    "\n",
    "      # Two NNs to Approximate Fluid, Solid Temperatures\n",
    "        self.approximate_solution_fluid = NeuralNet(input_dimension=self.domain_extrema.shape[0], output_dimension=1,\n",
    "                                              n_hidden_layers=7,\n",
    "                                              neurons=40,\n",
    "                                              regularization_param=0.01,\n",
    "                                              regularization_exp=2.,\n",
    "                                              retrain_seed=42)\n",
    "        self.approximate_solution_solid = NeuralNet(input_dimension=self.domain_extrema.shape[0], output_dimension=1,\n",
    "                                    n_hidden_layers=7,\n",
    "                                    neurons=40,\n",
    "                                    regularization_param=0.01,\n",
    "                                    regularization_exp=2.,\n",
    "                                    retrain_seed=42)\n",
    "\n",
    "\n",
    "      # Generator of Sobol sequences\n",
    "        self.soboleng = torch.quasirandom.SobolEngine(dimension=self.domain_extrema.shape[0], scramble = True)\n",
    "\n",
    "      # Training Sets S_tb, S_sb, S_int as Torch Dataloader\n",
    "        self.training_set_tb, self.training_set_sb, self.training_set_int, = self.assemble_datasets()\n",
    "\n",
    "    ################################################################################################\n",
    "\n",
    "    def convert(self, tens):\n",
    "        assert (tens.shape[1] == self.domain_extrema.shape[0])\n",
    "        return tens * (self.domain_extrema[:, 1] - self.domain_extrema[:, 0]) + self.domain_extrema[:, 0]\n",
    "\n",
    "\n",
    "    def apply_T_0(self, x):\n",
    "        return torch.full_like(x, self.T_0)\n",
    "\n",
    "\n",
    "    def apply_boundary_condition_fluid(self, t):\n",
    "        numerator = self.T_hot - self.T_0\n",
    "        denominator = 1 + torch.exp(-200*(t - 0.25))\n",
    "\n",
    "        return numerator/denominator + self.T_0\n",
    "\n",
    "    def create_sb_masks(self, inp_sb):\n",
    "        sb_0_mask = (inp_sb[:,1] == 0)\n",
    "        sb_L_mask = (inp_sb[:,1] == 1)\n",
    "\n",
    "        return sb_0_mask, sb_L_mask\n",
    "\n",
    "\n",
    "    ################################################################################################\n",
    "    # Function returning the input-output tensor required to assemble the training set S_tb corresponding to the temporal boundary\n",
    "    def add_temporal_boundary_points(self):\n",
    "        t0 = self.domain_extrema[0, 0]\n",
    "        input_tb = self.convert(self.soboleng.draw(self.n_tb))\n",
    "        input_tb[:, 0] = torch.full(input_tb[:, 0].shape, t0)\n",
    "        output_tb = self.apply_T_0(input_tb[:, 1]).reshape(-1, 1)\n",
    "\n",
    "        return input_tb, output_tb\n",
    "\n",
    "    # Function returning the input-output tensor required to assemble the training set S_sb corresponding to the spatial boundary\n",
    "    def add_spatial_boundary_points(self):\n",
    "        x0 = self.domain_extrema[1, 0]\n",
    "        xL = self.domain_extrema[1, 1]\n",
    "\n",
    "        input_sb = self.convert(self.soboleng.draw(self.n_sb))\n",
    "\n",
    "        input_sb_0 = torch.clone(input_sb)\n",
    "        input_sb_0[:, 1] = torch.full(input_sb_0[:, 1].shape, x0)\n",
    "\n",
    "        input_sb_L = torch.clone(input_sb)\n",
    "        input_sb_L[:, 1] = torch.full(input_sb_L[:, 1].shape, xL)\n",
    "\n",
    "        output_sb_0 = self.apply_boundary_condition_fluid(input_sb_0[:, 0]).reshape(-1, 1)\n",
    "        output_sb_L = torch.zeros((input_sb.shape[0], 1))\n",
    "\n",
    "        return torch.cat([input_sb_0, input_sb_L], 0), torch.cat([output_sb_0, output_sb_L], 0)\n",
    "\n",
    "    #  Function returning the input-output tensor required to assemble the training set S_int corresponding to the interior domain where the PDE is enforced\n",
    "    def add_interior_points(self):\n",
    "        input_int = self.convert(self.soboleng.draw(self.n_int))\n",
    "        output_int = torch.zeros((input_int.shape[0], 1))\n",
    "        return input_int, output_int\n",
    "\n",
    "\n",
    "    # Function returning the training sets S_sb, S_tb, S_int as dataloader\n",
    "    def assemble_datasets(self):\n",
    "        input_tb, output_tb = self.add_temporal_boundary_points()  # S_tb\n",
    "        input_sb, output_sb = self.add_spatial_boundary_points()  # S_sb\n",
    "        input_int, output_int = self.add_interior_points()  # S_int\n",
    "\n",
    "\n",
    "        training_set_tb = DataLoader(torch.utils.data.TensorDataset(input_tb, output_tb), batch_size=self.n_tb, shuffle=False)\n",
    "        training_set_sb = DataLoader(torch.utils.data.TensorDataset(input_sb, output_sb), batch_size=2 * self.space_dimensions * self.n_sb, shuffle=False) #2 * self.space_dimensions * self.n_sb\n",
    "        training_set_int = DataLoader(torch.utils.data.TensorDataset(input_int, output_int), batch_size=self.n_int, shuffle=False)\n",
    "\n",
    "        return training_set_tb, training_set_sb, training_set_int\n",
    "\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "    #   Compute Temporal Boundary Residuals\n",
    "    def compute_temporal_boundary_residual(self, input_tb, output_tb):\n",
    "        assert(torch.all(input_tb[:,0] == 0))\n",
    "        T_f = self.approximate_solution_fluid(input_tb).reshape(-1,)\n",
    "        T_s = self.approximate_solution_fluid(input_tb).reshape(-1,)\n",
    "\n",
    "        temp_tb_train = output_tb.reshape(-1,)\n",
    "\n",
    "        assert(T_f.shape == T_s.shape == temp_tb_train.shape)\n",
    "        residual_tb_fluid = temp_tb_train - T_f\n",
    "        residual_tb_solid = temp_tb_train - T_s\n",
    "        residual_tb = torch.cat((residual_tb_fluid, residual_tb_solid), dim=0)\n",
    "\n",
    "        return residual_tb.reshape(-1,)\n",
    "\n",
    "    #   Compute Spatial Boundary Residuals\n",
    "    def compute_spatial_boundary_residual(self, input_sb, output_sb):\n",
    "        input_sb.requires_grad = True\n",
    "        T_f = self.approximate_solution_fluid(input_sb).reshape(-1,)\n",
    "        T_s = self.approximate_solution_solid(input_sb).reshape(-1,)\n",
    "\n",
    "        grad_T_f = torch.autograd.grad(T_f.sum(), input_sb, create_graph=True)[0]\n",
    "        grad_T_f_x = grad_T_f[:, 1]\n",
    "        grad_T_s = torch.autograd.grad(T_s.sum(), input_sb, create_graph=True)[0]\n",
    "        grad_T_s_x = grad_T_s[:, 1]\n",
    "        assert(T_f.shape == T_s.shape and T_f.shape == grad_T_f_x.shape and T_s.shape == grad_T_s_x.shape)\n",
    "\n",
    "        mask_sb_0, mask_sb_L = self.create_sb_masks(input_sb)\n",
    "        residual_sb_fluid = torch.zeros_like(T_f)\n",
    "        temp_train_sb = output_sb.squeeze(1)\n",
    "        residual_sb_fluid[mask_sb_0] = temp_train_sb[mask_sb_0] - T_f[mask_sb_0]\n",
    "        residual_sb_fluid[mask_sb_L] = grad_T_f_x[mask_sb_L]\n",
    "        residual_sb_solid = grad_T_s_x\n",
    "\n",
    "        residual_sb = torch.cat((residual_sb_fluid, residual_sb_solid), dim=0)\n",
    "        return residual_sb.reshape(-1, )\n",
    "\n",
    "    #   Compute Interior Residuals\n",
    "    def compute_interior_residual(self, input_int):\n",
    "        input_int.requires_grad = True\n",
    "        T_f = self.approximate_solution_fluid(input_int).reshape(-1,)\n",
    "        T_s = self.approximate_solution_solid(input_int).reshape(-1,)\n",
    "\n",
    "        grad_T_f = torch.autograd.grad(T_f.sum(), input_int, create_graph=True)[0]\n",
    "        grad_T_f_t = grad_T_f[:, 0]\n",
    "        grad_T_f_x = grad_T_f[:, 1]\n",
    "        grad_T_f_xx = torch.autograd.grad(grad_T_f_x.sum(), input_int, create_graph=True)[0][:, 1]\n",
    "\n",
    "        grad_T_s = torch.autograd.grad(T_s.sum(), input_int, create_graph=True)[0]\n",
    "        grad_T_s_t = grad_T_s[:, 0]\n",
    "        grad_T_s_x = grad_T_s[:, 1]\n",
    "        grad_T_s_xx = torch.autograd.grad(grad_T_s_x.sum(), input_int, create_graph=True)[0][:, 1]\n",
    "\n",
    "        assert(T_f.shape == T_s.shape and T_f.shape == grad_T_f_t.shape and T_f.shape == grad_T_s_x.shape and T_f.shape == grad_T_s_xx.shape )\n",
    "\n",
    "\n",
    "        residual_1 = grad_T_f_t + self.U_f*grad_T_f_x - self.alpha_f*grad_T_f_xx + self.h_f*(T_f-T_s)\n",
    "        residual_2 = grad_T_s_t - self.alpha_s*grad_T_s_xx - self.h_s*(T_f - T_s)\n",
    "\n",
    "        residual_int = torch.cat((residual_1, residual_2), dim=0)\n",
    "\n",
    "        return residual_int.reshape(-1, )\n",
    "\n",
    "\n",
    "    #   Function to compute the total loss (weighted sum of spatial boundary loss, temporal boundary loss and interior loss)\n",
    "    def compute_loss(self, inp_train_tb, T_train_tb, inp_train_sb, T_train_sb, inp_train_int, T_train_int, verbose=True):\n",
    "      # Temporal Boundary Residuals\n",
    "        r_tb = self.compute_temporal_boundary_residual(inp_train_tb, T_train_tb)\n",
    "\n",
    "      # Spatial Boundary Residuals\n",
    "        r_sb = self.compute_spatial_boundary_residual(inp_train_sb, T_train_sb)\n",
    "\n",
    "      # Interior Residuals\n",
    "        r_int = self.compute_interior_residual(inp_train_int)\n",
    "\n",
    "        loss_tb = torch.mean(abs(r_tb) ** 2)\n",
    "        loss_sb = torch.mean(abs(r_sb) ** 2)\n",
    "        loss_int = torch.mean(abs(r_int) ** 2)\n",
    "\n",
    "        loss_function = loss_tb + loss_sb\n",
    "\n",
    "        loss = torch.log10(150*(loss_tb + loss_sb) + loss_int)\n",
    "        if verbose: print(\"Total loss: \", round(loss.item(), 4), \"| PDE Loss: \", round(torch.log10(loss_int).item(), 4), \"| Function Loss: \", round(torch.log10(loss_function).item(), 4))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    ################################################################################################\n",
    "    def fit(self, num_epochs, optimizer, verbose=True):\n",
    "        history = list()\n",
    "        \"\"\"\n",
    "        if os.path.exists('/content/drive/MyDrive/parameters_pinn_task1_f'):\n",
    "            checkpoint_f = torch.load('/content/drive/MyDrive/parameters_pinn_task1_f')\n",
    "            self.approximate_solution_fluid.load_state_dict(checkpoint_f['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint_f['optimizer'])\n",
    "            checkpoint_s = torch.load('/content/drive/MyDrive/parameters_pinn_task1_s')\n",
    "            self.approximate_solution_solid.load_state_dict(checkpoint_s['state_dict'])\n",
    "        \"\"\"\n",
    "\n",
    "        # Loop over epochs\n",
    "        for epoch in range(num_epochs):\n",
    "            if verbose: print(\"################################ \", epoch, \" ################################\")\n",
    "\n",
    "            for j, ((inp_train_tb, u_train_tb), (inp_train_sb, u_train_sb), (inp_train_int, u_train_int)) in enumerate(zip(self.training_set_tb, self.training_set_sb, self.training_set_int)):\n",
    "                def closure():\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = self.compute_loss(inp_train_tb, u_train_tb, inp_train_sb, u_train_sb, inp_train_int, u_train_int, verbose=True)\n",
    "                    loss.backward()\n",
    "\n",
    "                    history.append(loss.item())\n",
    "                    return loss\n",
    "\n",
    "                optimizer.step(closure=closure)\n",
    "        \"\"\"\n",
    "        self.best_parameters_f = {'state_dict': self.approximate_solution_fluid.state_dict(), 'optimizer': optimizer.state_dict(),}\n",
    "        torch.save(self.best_parameters_f, '/content/drive/MyDrive/parameters_pinn_task1_f')\n",
    "        self.best_parameters_s = {'state_dict': self.approximate_solution_solid.state_dict(), 'optimizer': optimizer.state_dict(),}\n",
    "        torch.save(self.best_parameters_s, '/content/drive/MyDrive/parameters_pinn_task1_s')\n",
    "        \"\"\"\n",
    "\n",
    "        print('Final Loss: ', history[-1])\n",
    "\n",
    "        return history\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AES0I12C6aWT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_int = 4096\n",
    "n_sb = 512\n",
    "n_tb = 1024\n",
    "\n",
    "pinn = Pinns(n_int, n_sb, n_tb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4tWjQkXo6aWU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "optimizer_LBFGS = optim.LBFGS(list(pinn.approximate_solution_fluid.parameters()) + list(pinn.approximate_solution_solid.parameters()),\n",
    "                              lr=float(0.5),\n",
    "                              max_iter=20000,\n",
    "                              max_eval=50000,\n",
    "                              history_size=150,\n",
    "                              line_search_fn=\"strong_wolfe\",\n",
    "                              tolerance_change=1.0 * np.finfo(float).eps)\n",
    "optimizer_ADAM = optim.Adam(list(pinn.approximate_solution_fluid.parameters()) + list(pinn.approximate_solution_solid.parameters()),\n",
    "                              lr=float(0.005))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 819
    },
    "id": "z12E1evS6aWU",
    "outputId": "e33f15af-608a-4263-aad0-0f91ec1cc9f3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################  0  ################################\n",
      "Total loss:  3.6972 | PDE Loss:  0.9728 | Function Loss:  1.5203\n",
      "Total loss:  3.6386 | PDE Loss:  0.9942 | Function Loss:  1.4615\n",
      "Total loss:  3.2245 | PDE Loss:  1.275 | Function Loss:  1.0435\n",
      "Total loss:  2.969 | PDE Loss:  1.7604 | Function Loss:  0.7652\n",
      "Total loss:  2.7876 | PDE Loss:  1.5638 | Function Loss:  0.5847\n",
      "Total loss:  2.614 | PDE Loss:  1.3226 | Function Loss:  0.4151\n",
      "Total loss:  2.4418 | PDE Loss:  1.2735 | Function Loss:  0.2352\n",
      "Total loss:  3.0991 | PDE Loss:  2.4862 | Function Loss:  0.8016\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m hist \u001b[38;5;241m=\u001b[39m \u001b[43mpinn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_LBFGS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m)\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mgrid(\u001b[38;5;28;01mTrue\u001b[39;00m, which\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m\"\u001b[39m, ls\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 233\u001b[0m, in \u001b[0;36mPinns.fit\u001b[1;34m(self, num_epochs, optimizer, verbose)\u001b[0m\n\u001b[0;32m    230\u001b[0m             history\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m    231\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[1;32m--> 233\u001b[0m         \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;124;03mself.best_parameters_f = {'state_dict': self.approximate_solution_fluid.state_dict(), 'optimizer': optimizer.state_dict(),}\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;124;03mtorch.save(self.best_parameters_f, '/content/drive/MyDrive/parameters_pinn_task1_f')\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;124;03mself.best_parameters_s = {'state_dict': self.approximate_solution_solid.state_dict(), 'optimizer': optimizer.state_dict(),}\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03mtorch.save(self.best_parameters_s, '/content/drive/MyDrive/parameters_pinn_task1_s')\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinal Loss: \u001b[39m\u001b[38;5;124m'\u001b[39m, history[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\optim\\lbfgs.py:426\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    423\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobj_func\u001b[39m(x, t, d):\n\u001b[0;32m    424\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_directional_evaluate(closure, x, t, d)\n\u001b[1;32m--> 426\u001b[0m     loss, flat_grad, t, ls_func_evals \u001b[38;5;241m=\u001b[39m \u001b[43m_strong_wolfe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgtd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_grad(t, d)\n\u001b[0;32m    429\u001b[0m opt_cond \u001b[38;5;241m=\u001b[39m flat_grad\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m tolerance_grad\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\optim\\lbfgs.py:50\u001b[0m, in \u001b[0;36m_strong_wolfe\u001b[1;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[0m\n\u001b[0;32m     48\u001b[0m g \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39mclone(memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcontiguous_format)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# evaluate objective and gradient using initial step\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m f_new, g_new \u001b[38;5;241m=\u001b[39m \u001b[43mobj_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m ls_func_evals \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     52\u001b[0m gtd_new \u001b[38;5;241m=\u001b[39m g_new\u001b[38;5;241m.\u001b[39mdot(d)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\optim\\lbfgs.py:424\u001b[0m, in \u001b[0;36mLBFGS.step.<locals>.obj_func\u001b[1;34m(x, t, d)\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobj_func\u001b[39m(x, t, d):\n\u001b[1;32m--> 424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_directional_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\optim\\lbfgs.py:278\u001b[0m, in \u001b[0;36mLBFGS._directional_evaluate\u001b[1;34m(self, closure, x, t, d)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_directional_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, closure, x, t, d):\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_grad(t, d)\n\u001b[1;32m--> 278\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    279\u001b[0m     flat_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gather_flat_grad()\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_param(x)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[3], line 228\u001b[0m, in \u001b[0;36mPinns.fit.<locals>.closure\u001b[1;34m()\u001b[0m\n\u001b[0;32m    226\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    227\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(inp_train_tb, u_train_tb, inp_train_sb, u_train_sb, inp_train_int, u_train_int, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 228\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    230\u001b[0m history\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\autograd\\__init__.py:204\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    201\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 204\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hist = pinn.fit(num_epochs=n_epochs,\n",
    "                optimizer=optimizer_LBFGS,\n",
    "                verbose=True)\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "plt.grid(True, which=\"both\", ls=\":\")\n",
    "plt.plot(np.arange(1, len(hist) + 1), hist, label=\"Train Loss\")\n",
    "plt.xscale(\"log\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 992
    },
    "id": "gH0QWLch6aWV",
    "outputId": "5ec007cd-d1b0-4ff8-bf43-948d96d179db",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "    # Plot results with colour\n",
    "\n",
    "\n",
    "def plotting():\n",
    "  inputs = pinn.soboleng.draw(100000)\n",
    "  inputs = pinn.convert(inputs)\n",
    "  output_fluid = pinn.approximate_solution_fluid(inputs)\n",
    "  output_solid = pinn.approximate_solution_solid(inputs)\n",
    "\n",
    "  fig, axs = plt.subplots(1, 2, figsize=(16, 8), dpi=150)\n",
    "  im1 = axs[0].scatter(inputs[:, 1].detach(), inputs[:, 0].detach(), c=output_fluid.detach(), cmap=\"jet\", s=2)\n",
    "  axs[0].set_xlabel(\"x\")\n",
    "  axs[0].set_ylabel(\"t\")\n",
    "  plt.colorbar(im1, ax=axs[0])\n",
    "  axs[0].grid(True, which=\"both\", ls=\":\")\n",
    "  im2 = axs[1].scatter(inputs[:, 1].detach(), inputs[:, 0].detach(), c=output_solid.detach(), cmap=\"jet\", s=2)\n",
    "  axs[1].set_xlabel(\"x\")\n",
    "  axs[1].set_ylabel(\"t\")\n",
    "  plt.colorbar(im2, ax=axs[1])\n",
    "  axs[1].grid(True, which=\"both\", ls=\":\")\n",
    "  axs[0].set_title(\"Temp Fluid\")\n",
    "  axs[1].set_title(\"Temp Solid\")\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plotting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YkwVrh81NqBg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

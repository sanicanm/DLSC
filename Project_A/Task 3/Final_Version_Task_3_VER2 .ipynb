{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28780,
     "status": "ok",
     "timestamp": 1686836964078,
     "user": {
      "displayName": "matth (matth)",
      "userId": "10601060683445076648"
     },
     "user_tz": -120
    },
    "id": "0CmtecIjNXFk",
    "outputId": "ca82ba16-18c1-4a53-f988-332a1c18c08a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1686836964080,
     "user": {
      "displayName": "matth (matth)",
      "userId": "10601060683445076648"
     },
     "user_tz": -120
    },
    "id": "que-OA-MNemp"
   },
   "outputs": [],
   "source": [
    "def activation(name):\n",
    "    if name in ['tanh', 'Tanh']:\n",
    "        return nn.Tanh()\n",
    "    elif name in ['relu', 'ReLU']:\n",
    "        return nn.ReLU(inplace=True)\n",
    "    elif name in ['lrelu', 'LReLU']:\n",
    "        return nn.LeakyReLU(inplace=True)\n",
    "    elif name in ['sigmoid', 'Sigmoid']:\n",
    "        return nn.Sigmoid()\n",
    "    elif name in ['softplus', 'Softplus']:\n",
    "        return nn.Softplus(beta=4)\n",
    "    elif name in ['celu', 'CeLU']:\n",
    "        return nn.CELU()\n",
    "    elif name in ['elu']:\n",
    "        return nn.ELU()\n",
    "    elif name in ['mish']:\n",
    "        return nn.Mish()\n",
    "    else:\n",
    "        raise ValueError('Unknown activation function')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################\n",
    "#  1d fourier layer\n",
    "################################################################\n",
    "class SpectralConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes1):\n",
    "        super(SpectralConv1d, self).__init__()\n",
    "\n",
    "        self.regularization_exp = 1\n",
    "        self.regularization_param = 0.0000001\n",
    "\n",
    "        \"\"\"\n",
    "        1D Fourier layer. It does FFT, linear transform, and Inverse FFT.\n",
    "        \"\"\"\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes1 = modes1\n",
    "\n",
    "        self.scale = (1 / (in_channels * out_channels))\n",
    "        self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, dtype=torch.complex128))\n",
    "\n",
    "\n",
    "    # Complex multiplication\n",
    "    def compl_mul1d(self, input, weights):\n",
    "        #print(\"einsum: input, weights\", input.shape, weights.shape)\n",
    "        # (batch, in_channel, x ), (in_channel, out_channel, x) -> (batch, out_channel, x)\n",
    "\n",
    "        return torch.einsum(\"bix,iox->box\", input, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "        # x.shape == [batch_size, in_channels, number of grid points]\n",
    "        # hint: use torch.fft library torch.fft.rfft\n",
    "        # use DFT to approximate the fourier transform\n",
    "\n",
    "        # Compute Fourier coefficients\n",
    "        #print(\"x before fft\", x.shape)\n",
    "        x_ft = torch.fft.rfft(x)\n",
    "        #print(\"x after fft\", x.shape)\n",
    "\n",
    "        # Multiply relevant Fourier modes\n",
    "        out_ft = torch.zeros(batchsize, self.out_channels, x.size(-1) // 2 + 1, device=x.device, dtype=torch.complex128)\n",
    "        #print(\"outft shape\", out_ft.shape)\n",
    "        out_ft[:, :, :self.modes1] = self.compl_mul1d(x_ft[:, :,:self.modes1], self.weights1)\n",
    "\n",
    "        # Return to physical space\n",
    "        x = torch.fft.irfft(out_ft, n=x.size(-1))\n",
    "        return x\n",
    "\n",
    "    def regularization(self):\n",
    "        reg_loss = 0\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                reg_loss = reg_loss + torch.norm(param, self.regularization_exp)\n",
    "        return self.regularization_param * reg_loss\n",
    "\n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "class FNO1d(nn.Module):\n",
    "    def __init__(self, modes, width):\n",
    "        super(FNO1d, self).__init__()\n",
    "        self.modes1 = modes\n",
    "        self.width = width\n",
    "        self.padding = 1  # pad the domain if input is non-periodic\n",
    "        self.linear_p = nn.Linear(3, self.width, dtype=torch.float64)\n",
    "\n",
    "        self.spect1 = SpectralConv1d(self.width, self.width, self.modes1)\n",
    "        self.spect2 = SpectralConv1d(self.width, self.width, self.modes1)\n",
    "        self.spect3 = SpectralConv1d(self.width, self.width, self.modes1)\n",
    "        self.lin0 = nn.Conv1d(self.width, self.width, 1, dtype = torch.float64)\n",
    "        self.lin1 = nn.Conv1d(self.width, self.width, 1, dtype = torch.float64)\n",
    "        self.lin2 = nn.Conv1d(self.width, self.width, 1, dtype = torch.float64)\n",
    "\n",
    "        self.linear_q = nn.Linear(self.width, 32, dtype = torch.float64)\n",
    "        self.output_layer = nn.Linear(32, 2, dtype = torch.float64)\n",
    "\n",
    "        self.activation = torch.nn.Tanh()\n",
    "\n",
    "    def fourier_layer(self, x, spectral_layer, conv_layer):\n",
    "        return self.activation(spectral_layer(x) + conv_layer(x))\n",
    "\n",
    "    def linear_layer(self, x, linear_transformation):\n",
    "        return self.activation(linear_transformation(x))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # grid = self.get_grid(x.shape, x.device)\n",
    "        # x = torch.cat((x, grid), dim=-1)\n",
    "        #print(\"x before linear_Ã¼\",x.shape)\n",
    "        x = self.linear_p(x)\n",
    "        #print(\"x after linear_p\",x.shape)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        #print(\"x after permute\",x.shape)\n",
    "\n",
    "        #x = F.pad(x, [0, self.padding])  # pad the domain if input is non-periodic\n",
    "\n",
    "        x = self.fourier_layer(x, self.spect1, self.lin0)\n",
    "        x = self.fourier_layer(x, self.spect2, self.lin1)\n",
    "        x = self.fourier_layer(x, self.spect3, self.lin2)\n",
    "\n",
    "        #x = x[..., :-self.padding]  # pad the domain if input is non-periodic\n",
    "\n",
    "        x = x.permute(0, 2, 1)\n",
    "        #print(\"x after permute\",x.shape)\n",
    "\n",
    "        x = self.linear_layer(x, self.linear_q)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "    def compute_loss(self, pred, train):\n",
    "        l = torch.nn.MSELoss()\n",
    "        loss = l(pred, train) + self.spect1.regularization() + self.spect2.regularization() + self.spect3.regularization()\n",
    "        #loss = tuple(loss)\n",
    "        return loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 862,
     "status": "ok",
     "timestamp": 1686836964931,
     "user": {
      "displayName": "matth (matth)",
      "userId": "10601060683445076648"
     },
     "user_tz": -120
    },
    "id": "QJfGCSCvNi6G",
    "outputId": "947717a7-8c79-400e-95b9-8bf670f01528"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([210, 3])\n",
      "torch.Size([176, 35, 3])\n",
      "torch.Size([141, 35, 3]) torch.Size([141, 35, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "n_train = 106\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\matth\\OneDrive\\Documents\\TrainingData.txt')\n",
    "data_read= df.iloc[:,0:3].values\n",
    "\n",
    "data_read = torch.tensor(data_read).to(torch.float64)\n",
    "print(data_read.shape)\n",
    "\n",
    "Time_Column = torch.clone(data_read[:, 0])\n",
    "Tf_Column = torch.clone(data_read[:, 1])\n",
    "Ts_Column = torch.clone(data_read[:, 2])\n",
    "\n",
    "\n",
    "data_read[:, 0] = Tf_Column\n",
    "data_read[:, 1] = Ts_Column\n",
    "data_read[:, 2] = Time_Column\n",
    "\n",
    "\n",
    "max_Tf = torch.max(data_read[:,0])\n",
    "max_Ts = torch.max(data_read[:,1])\n",
    "\n",
    "\n",
    "data_read[:,0] /= max_Tf\n",
    "data_read[:,1] /= max_Ts\n",
    "data_read[:,2] /= 602168.58\n",
    "\n",
    "\n",
    "data_read = data_read.unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "data_altered = torch.tensor((), dtype=torch.float64).to(device)\n",
    "WINDOW_SIZE = 35\n",
    "NUM_MEASUREMENTS = data_read.shape[1]\n",
    "\n",
    "n_intervals = NUM_MEASUREMENTS - (WINDOW_SIZE - 1)\n",
    "\n",
    "for i in range(0, n_intervals):\n",
    "    data_altered = torch.cat(\n",
    "        (data_altered, data_read[:, i : 35 + i]), dim=0)\n",
    "print(data_altered.shape)\n",
    "\n",
    "x_data = data_altered[0:141,:,0:3]\n",
    "y_data = data_altered[35:176,:,0:2]\n",
    "\n",
    "\n",
    "print(x_data.shape,y_data.shape)\n",
    "#print(\"read value is:\", data_read[:,173,0])\n",
    "#print(\"our value is: \", x_data[140, 34, 0])\n",
    "input_function_train = x_data#[:n_train, :]\n",
    "output_function_train = y_data#[:n_train, :]\n",
    "#input_function_test = x_data[n_train:, :]\n",
    "#output_function_test = y_data[n_train:, :]\n",
    "\n",
    "\n",
    "batch_size = 141\n",
    "\n",
    "training_set = DataLoader(TensorDataset(input_function_train, output_function_train), batch_size=batch_size, shuffle=True)\n",
    "#testing_set = DataLoader(TensorDataset(input_function_test, output_function_test), batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WxXDVVNaNpJA",
    "outputId": "9a2e038b-3bff-44ba-e667-af467ab58a89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### Epoch: 0  ######### Train Loss: -0.1324861098802973\n",
      "######### Epoch: 1  ######### Train Loss: -1.7336442430227017\n",
      "######### Epoch: 2  ######### Train Loss: -0.3276130919438893\n",
      "######### Epoch: 3  ######### Train Loss: -0.06188090754908181\n",
      "######### Epoch: 4  ######### Train Loss: -0.03466868437544145\n",
      "######### Epoch: 5  ######### Train Loss: -0.11501956501918939\n",
      "######### Epoch: 6  ######### Train Loss: -0.31195111037198386\n",
      "######### Epoch: 7  ######### Train Loss: -0.7507355289244905\n",
      "######### Epoch: 8  ######### Train Loss: -2.4340939767256002\n",
      "######### Epoch: 9  ######### Train Loss: -0.9398601758217736\n",
      "######### Epoch: 10  ######### Train Loss: -0.5435266871377394\n",
      "######### Epoch: 11  ######### Train Loss: -0.4107392125885409\n",
      "######### Epoch: 12  ######### Train Loss: -0.3777779195046619\n",
      "######### Epoch: 13  ######### Train Loss: -0.3991698449037402\n",
      "######### Epoch: 14  ######### Train Loss: -0.45656341376472637\n",
      "######### Epoch: 15  ######### Train Loss: -0.5417359284546279\n",
      "######### Epoch: 16  ######### Train Loss: -0.6523672303935701\n",
      "######### Epoch: 17  ######### Train Loss: -0.7918754916615324\n",
      "######### Epoch: 18  ######### Train Loss: -0.9738589670385479\n",
      "######### Epoch: 19  ######### Train Loss: -1.2344726965125512\n",
      "######### Epoch: 20  ######### Train Loss: -1.671982764328464\n",
      "######### Epoch: 21  ######### Train Loss: -2.2424852427415725\n",
      "######### Epoch: 22  ######### Train Loss: -1.7558733982689214\n",
      "######### Epoch: 23  ######### Train Loss: -1.6669793649223281\n",
      "######### Epoch: 24  ######### Train Loss: -1.7639189723532094\n",
      "######### Epoch: 25  ######### Train Loss: -2.093424535881699\n",
      "######### Epoch: 26  ######### Train Loss: -2.673542142494439\n",
      "######### Epoch: 27  ######### Train Loss: -1.6122807823722414\n",
      "######### Epoch: 28  ######### Train Loss: -1.3497226707191143\n",
      "######### Epoch: 29  ######### Train Loss: -1.4937685329749077\n",
      "######### Epoch: 30  ######### Train Loss: -1.8979497633295859\n",
      "######### Epoch: 31  ######### Train Loss: -1.9516775862769182\n",
      "######### Epoch: 32  ######### Train Loss: -1.8536391725367376\n",
      "######### Epoch: 33  ######### Train Loss: -2.0314921443825007\n",
      "######### Epoch: 34  ######### Train Loss: -2.585442544631013\n",
      "######### Epoch: 35  ######### Train Loss: -2.227899449978685\n",
      "######### Epoch: 36  ######### Train Loss: -2.0385573615015726\n",
      "######### Epoch: 37  ######### Train Loss: -2.180294022129173\n",
      "######### Epoch: 38  ######### Train Loss: -2.5104824775706884\n",
      "######### Epoch: 39  ######### Train Loss: -2.4169814529426668\n",
      "######### Epoch: 40  ######### Train Loss: -2.507859636747134\n",
      "######### Epoch: 41  ######### Train Loss: -3.159210462761204\n",
      "######### Epoch: 42  ######### Train Loss: -2.5490974884269564\n",
      "######### Epoch: 43  ######### Train Loss: -2.434682171077961\n",
      "######### Epoch: 44  ######### Train Loss: -2.605853217318337\n",
      "######### Epoch: 45  ######### Train Loss: -2.7832046318706403\n",
      "######### Epoch: 46  ######### Train Loss: -2.8218704746666385\n",
      "######### Epoch: 47  ######### Train Loss: -3.2878389008728552\n",
      "######### Epoch: 48  ######### Train Loss: -2.4931275064964393\n",
      "######### Epoch: 49  ######### Train Loss: -2.3084570389835686\n",
      "######### Epoch: 50  ######### Train Loss: -2.5399012652530275\n",
      "######### Epoch: 51  ######### Train Loss: -3.273011016230688\n",
      "######### Epoch: 52  ######### Train Loss: -2.718977528192644\n",
      "######### Epoch: 53  ######### Train Loss: -2.3519816863157605\n",
      "######### Epoch: 54  ######### Train Loss: -2.246922495313457\n",
      "######### Epoch: 55  ######### Train Loss: -2.2667162424094465\n",
      "######### Epoch: 56  ######### Train Loss: -2.3853842354046537\n",
      "######### Epoch: 57  ######### Train Loss: -2.6075653963751058\n",
      "######### Epoch: 58  ######### Train Loss: -2.8701012780945026\n",
      "######### Epoch: 59  ######### Train Loss: -2.847315582530409\n",
      "######### Epoch: 60  ######### Train Loss: -2.77121500547251\n",
      "######### Epoch: 61  ######### Train Loss: -2.929491254075932\n",
      "######### Epoch: 62  ######### Train Loss: -3.272658285071644\n",
      "######### Epoch: 63  ######### Train Loss: -2.9733236654174644\n",
      "######### Epoch: 64  ######### Train Loss: -2.947797621885909\n",
      "######### Epoch: 65  ######### Train Loss: -3.3224506665161178\n",
      "######### Epoch: 66  ######### Train Loss: -3.1798638523426757\n",
      "######### Epoch: 67  ######### Train Loss: -3.0335377341015213\n",
      "######### Epoch: 68  ######### Train Loss: -3.2116903905570258\n",
      "######### Epoch: 69  ######### Train Loss: -3.310941308323266\n",
      "######### Epoch: 70  ######### Train Loss: -3.3517936560699617\n",
      "######### Epoch: 71  ######### Train Loss: -3.5704722678804286\n",
      "######### Epoch: 72  ######### Train Loss: -3.4127196489329217\n",
      "######### Epoch: 73  ######### Train Loss: -3.8152811156704183\n",
      "######### Epoch: 74  ######### Train Loss: -3.444555141074555\n",
      "######### Epoch: 75  ######### Train Loss: -3.7264574682003535\n",
      "######### Epoch: 76  ######### Train Loss: -3.5039137765303456\n",
      "######### Epoch: 77  ######### Train Loss: -3.8654371555287868\n",
      "######### Epoch: 78  ######### Train Loss: -3.7695866339683075\n",
      "######### Epoch: 79  ######### Train Loss: -3.2649947913258814\n",
      "######### Epoch: 80  ######### Train Loss: -3.141275471069852\n",
      "######### Epoch: 81  ######### Train Loss: -3.6635391396185706\n",
      "######### Epoch: 82  ######### Train Loss: -3.1424606471689414\n",
      "######### Epoch: 83  ######### Train Loss: -2.99068464075422\n",
      "######### Epoch: 84  ######### Train Loss: -3.2769268149715605\n",
      "######### Epoch: 85  ######### Train Loss: -3.4727494709957076\n",
      "######### Epoch: 86  ######### Train Loss: -3.3080218331717903\n",
      "######### Epoch: 87  ######### Train Loss: -3.6962353300935193\n",
      "######### Epoch: 88  ######### Train Loss: -3.3852399366294224\n",
      "######### Epoch: 89  ######### Train Loss: -3.0827182232802928\n",
      "######### Epoch: 90  ######### Train Loss: -3.1546333864557\n",
      "######### Epoch: 91  ######### Train Loss: -3.47648929584214\n",
      "######### Epoch: 92  ######### Train Loss: -3.4549928547932636\n",
      "######### Epoch: 93  ######### Train Loss: -3.422216890150329\n",
      "######### Epoch: 94  ######### Train Loss: -3.8453509188829376\n",
      "######### Epoch: 95  ######### Train Loss: -3.3599670038679585\n",
      "######### Epoch: 96  ######### Train Loss: -3.18014308067387\n",
      "######### Epoch: 97  ######### Train Loss: -3.4383776283902407\n",
      "######### Epoch: 98  ######### Train Loss: -4.001775842778246\n",
      "######### Epoch: 99  ######### Train Loss: -4.023936627968271\n",
      "######### Epoch: 100  ######### Train Loss: -3.5323850072748937\n",
      "######### Epoch: 101  ######### Train Loss: -3.5707633903445632\n",
      "######### Epoch: 102  ######### Train Loss: -4.471877352260741\n",
      "######### Epoch: 103  ######### Train Loss: -3.919690181363103\n",
      "######### Epoch: 104  ######### Train Loss: -3.9991653659316895\n",
      "######### Epoch: 105  ######### Train Loss: -4.369008473390837\n",
      "######### Epoch: 106  ######### Train Loss: -4.222935632289134\n",
      "######### Epoch: 107  ######### Train Loss: -3.7956372140406236\n",
      "######### Epoch: 108  ######### Train Loss: -3.892931714916618\n",
      "######### Epoch: 109  ######### Train Loss: -4.074596549943272\n",
      "######### Epoch: 110  ######### Train Loss: -4.4256667952693425\n",
      "######### Epoch: 111  ######### Train Loss: -3.8732050389175936\n",
      "######### Epoch: 112  ######### Train Loss: -4.059607982530599\n",
      "######### Epoch: 113  ######### Train Loss: -3.8586509314364315\n",
      "######### Epoch: 114  ######### Train Loss: -3.915774008464303\n",
      "######### Epoch: 115  ######### Train Loss: -4.04319314662447\n",
      "######### Epoch: 116  ######### Train Loss: -4.175184811319104\n",
      "######### Epoch: 117  ######### Train Loss: -3.766906335366035\n",
      "######### Epoch: 118  ######### Train Loss: -3.502310401556415\n",
      "######### Epoch: 119  ######### Train Loss: -3.7510871746665373\n",
      "######### Epoch: 120  ######### Train Loss: -4.342027909619074\n",
      "######### Epoch: 121  ######### Train Loss: -4.2084165575634875\n",
      "######### Epoch: 122  ######### Train Loss: -4.085546982161395\n",
      "######### Epoch: 123  ######### Train Loss: -4.8241203020226475\n",
      "######### Epoch: 124  ######### Train Loss: -3.7524935969793995\n",
      "######### Epoch: 125  ######### Train Loss: -3.5254692396439475\n",
      "######### Epoch: 126  ######### Train Loss: -3.5414356269505545\n",
      "######### Epoch: 127  ######### Train Loss: -3.5760259483929273\n",
      "######### Epoch: 128  ######### Train Loss: -3.5989851077171044\n",
      "######### Epoch: 129  ######### Train Loss: -3.805593651234798\n",
      "######### Epoch: 130  ######### Train Loss: -4.687465214294455\n",
      "######### Epoch: 131  ######### Train Loss: -3.728442745795163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### Epoch: 132  ######### Train Loss: -3.3721004113795043\n",
      "######### Epoch: 133  ######### Train Loss: -3.233150608130064\n",
      "######### Epoch: 134  ######### Train Loss: -3.170395875358696\n",
      "######### Epoch: 135  ######### Train Loss: -3.152727471177277\n",
      "######### Epoch: 136  ######### Train Loss: -3.1823505034438035\n",
      "######### Epoch: 137  ######### Train Loss: -3.2758544822030133\n",
      "######### Epoch: 138  ######### Train Loss: -3.464768514255291\n",
      "######### Epoch: 139  ######### Train Loss: -3.8073826959409236\n",
      "######### Epoch: 140  ######### Train Loss: -4.19525231827208\n",
      "######### Epoch: 141  ######### Train Loss: -4.282929430631962\n",
      "######### Epoch: 142  ######### Train Loss: -4.023697808974123\n",
      "######### Epoch: 143  ######### Train Loss: -3.960423522579649\n",
      "######### Epoch: 144  ######### Train Loss: -4.044586588142116\n",
      "######### Epoch: 145  ######### Train Loss: -4.214382612561822\n",
      "######### Epoch: 146  ######### Train Loss: -4.476892555134925\n",
      "######### Epoch: 147  ######### Train Loss: -4.4697163614032505\n",
      "######### Epoch: 148  ######### Train Loss: -4.317199346757931\n",
      "######### Epoch: 149  ######### Train Loss: -4.521007052751467\n",
      "######### Epoch: 150  ######### Train Loss: -4.690175483618504\n",
      "######### Epoch: 151  ######### Train Loss: -4.522153736216876\n",
      "######### Epoch: 152  ######### Train Loss: -4.213931681052716\n",
      "######### Epoch: 153  ######### Train Loss: -4.505392453072062\n",
      "######### Epoch: 154  ######### Train Loss: -4.152344035869676\n",
      "######### Epoch: 155  ######### Train Loss: -4.34960433849417\n",
      "######### Epoch: 156  ######### Train Loss: -4.168734282791837\n",
      "######### Epoch: 157  ######### Train Loss: -4.415431402625919\n",
      "######### Epoch: 158  ######### Train Loss: -4.059400744144727\n",
      "######### Epoch: 159  ######### Train Loss: -4.245523465469755\n",
      "######### Epoch: 160  ######### Train Loss: -3.8998771153476337\n",
      "######### Epoch: 161  ######### Train Loss: -3.697864608190522\n",
      "######### Epoch: 162  ######### Train Loss: -4.0722412457006465\n",
      "######### Epoch: 163  ######### Train Loss: -4.035351651840818\n",
      "######### Epoch: 164  ######### Train Loss: -3.81766402192637\n",
      "######### Epoch: 165  ######### Train Loss: -4.404103539482677\n",
      "######### Epoch: 166  ######### Train Loss: -3.5868622227443487\n",
      "######### Epoch: 167  ######### Train Loss: -3.108423960422745\n",
      "######### Epoch: 168  ######### Train Loss: -2.923114782248191\n",
      "######### Epoch: 169  ######### Train Loss: -2.855336485883543\n",
      "######### Epoch: 170  ######### Train Loss: -2.8624251737620248\n",
      "######### Epoch: 171  ######### Train Loss: -2.9357713395539062\n",
      "######### Epoch: 172  ######### Train Loss: -3.0893336033120327\n",
      "######### Epoch: 173  ######### Train Loss: -3.379093613269056\n",
      "######### Epoch: 174  ######### Train Loss: -4.015980127794181\n",
      "######### Epoch: 175  ######### Train Loss: -3.8110195357502286\n",
      "######### Epoch: 176  ######### Train Loss: -3.3545778771673014\n",
      "######### Epoch: 177  ######### Train Loss: -3.211689282262254\n",
      "######### Epoch: 178  ######### Train Loss: -3.2240584324088246\n",
      "######### Epoch: 179  ######### Train Loss: -3.3800552237862522\n",
      "######### Epoch: 180  ######### Train Loss: -3.7765864117729437\n",
      "######### Epoch: 181  ######### Train Loss: -4.311990395374223\n",
      "######### Epoch: 182  ######### Train Loss: -3.891887014060364\n",
      "######### Epoch: 183  ######### Train Loss: -3.938758542672639\n",
      "######### Epoch: 184  ######### Train Loss: -4.598082743304929\n",
      "######### Epoch: 185  ######### Train Loss: -3.8048608791930856\n",
      "######### Epoch: 186  ######### Train Loss: -3.5176487177483136\n",
      "######### Epoch: 187  ######### Train Loss: -3.559284330892193\n",
      "######### Epoch: 188  ######### Train Loss: -3.973598141726811\n",
      "######### Epoch: 189  ######### Train Loss: -4.318315216334455\n",
      "######### Epoch: 190  ######### Train Loss: -4.169757939440594\n",
      "######### Epoch: 191  ######### Train Loss: -4.8624103927359466\n",
      "######### Epoch: 192  ######### Train Loss: -4.672871541739551\n",
      "######### Epoch: 193  ######### Train Loss: -4.1970509114964765\n",
      "######### Epoch: 194  ######### Train Loss: -4.2595297703460515\n",
      "######### Epoch: 195  ######### Train Loss: -4.428441627074652\n",
      "######### Epoch: 196  ######### Train Loss: -4.481682049733345\n",
      "######### Epoch: 197  ######### Train Loss: -4.226799845741515\n",
      "######### Epoch: 198  ######### Train Loss: -4.257634153118765\n",
      "######### Epoch: 199  ######### Train Loss: -4.626100490287165\n",
      "######### Epoch: 200  ######### Train Loss: -5.1735705636923965\n",
      "######### Epoch: 201  ######### Train Loss: -4.408679281454977\n",
      "######### Epoch: 202  ######### Train Loss: -4.919336427527671\n",
      "######### Epoch: 203  ######### Train Loss: -3.7487249775834237\n",
      "######### Epoch: 204  ######### Train Loss: -3.2940130126757095\n",
      "######### Epoch: 205  ######### Train Loss: -3.105350743737141\n",
      "######### Epoch: 206  ######### Train Loss: -3.028637536080453\n",
      "######### Epoch: 207  ######### Train Loss: -3.023541270223983\n",
      "######### Epoch: 208  ######### Train Loss: -3.079822562272004\n",
      "######### Epoch: 209  ######### Train Loss: -3.2054394871884866\n",
      "######### Epoch: 210  ######### Train Loss: -3.4366008802095314\n",
      "######### Epoch: 211  ######### Train Loss: -3.9030378818538707\n",
      "######### Epoch: 212  ######### Train Loss: -4.606651264493513\n",
      "######### Epoch: 213  ######### Train Loss: -4.120261792461713\n",
      "######### Epoch: 214  ######### Train Loss: -4.175764933417118\n",
      "######### Epoch: 215  ######### Train Loss: -4.702794287872135\n",
      "######### Epoch: 216  ######### Train Loss: -4.25570363255694\n",
      "######### Epoch: 217  ######### Train Loss: -4.216296673145163\n",
      "######### Epoch: 218  ######### Train Loss: -4.569395720161519\n",
      "######### Epoch: 219  ######### Train Loss: -4.46046314485433\n",
      "######### Epoch: 220  ######### Train Loss: -4.3518865654748184\n",
      "######### Epoch: 221  ######### Train Loss: -4.50078829431015\n",
      "######### Epoch: 222  ######### Train Loss: -4.778795944043641\n",
      "######### Epoch: 223  ######### Train Loss: -4.83457856117697\n",
      "######### Epoch: 224  ######### Train Loss: -4.321229875764922\n",
      "######### Epoch: 225  ######### Train Loss: -4.095801912651788\n",
      "######### Epoch: 226  ######### Train Loss: -4.357621194685503\n",
      "######### Epoch: 227  ######### Train Loss: -4.720948023714339\n",
      "######### Epoch: 228  ######### Train Loss: -4.527709840017049\n",
      "######### Epoch: 229  ######### Train Loss: -4.491270190985926\n",
      "######### Epoch: 230  ######### Train Loss: -5.032113563926393\n",
      "######### Epoch: 231  ######### Train Loss: -4.945384814140564\n",
      "######### Epoch: 232  ######### Train Loss: -4.324593195362502\n",
      "######### Epoch: 233  ######### Train Loss: -4.215541362718009\n",
      "######### Epoch: 234  ######### Train Loss: -4.729386267627565\n",
      "######### Epoch: 235  ######### Train Loss: -4.319241190641243\n",
      "######### Epoch: 236  ######### Train Loss: -4.05193329462322\n",
      "######### Epoch: 237  ######### Train Loss: -4.068160984882232\n",
      "######### Epoch: 238  ######### Train Loss: -4.225713596962989\n",
      "######### Epoch: 239  ######### Train Loss: -4.548212347980021\n",
      "######### Epoch: 240  ######### Train Loss: -4.486861216584786\n",
      "######### Epoch: 241  ######### Train Loss: -4.33314228954665\n",
      "######### Epoch: 242  ######### Train Loss: -4.710361162702979\n",
      "######### Epoch: 243  ######### Train Loss: -4.382869068368753\n",
      "######### Epoch: 244  ######### Train Loss: -4.255214266900998\n",
      "######### Epoch: 245  ######### Train Loss: -4.561010413611962\n",
      "######### Epoch: 246  ######### Train Loss: -4.631643950902097\n",
      "######### Epoch: 247  ######### Train Loss: -4.472119861089922\n",
      "######### Epoch: 248  ######### Train Loss: -4.555604920111039\n",
      "######### Epoch: 249  ######### Train Loss: -5.097834464725658\n",
      "######### Epoch: 250  ######### Train Loss: -5.563368857238332\n",
      "######### Epoch: 251  ######### Train Loss: -4.59539980454845\n",
      "######### Epoch: 252  ######### Train Loss: -4.844234741052057\n",
      "######### Epoch: 253  ######### Train Loss: -4.642957644151222\n",
      "######### Epoch: 254  ######### Train Loss: -4.4700255461475695\n",
      "######### Epoch: 255  ######### Train Loss: -4.865115704273222\n",
      "######### Epoch: 256  ######### Train Loss: -4.331456642915267\n",
      "######### Epoch: 257  ######### Train Loss: -3.9597900836891733\n",
      "######### Epoch: 258  ######### Train Loss: -3.9243556820662944\n",
      "######### Epoch: 259  ######### Train Loss: -4.089367715735477\n",
      "######### Epoch: 260  ######### Train Loss: -4.51998409442695\n",
      "######### Epoch: 261  ######### Train Loss: -4.773363583197906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### Epoch: 262  ######### Train Loss: -4.425990951197386\n",
      "######### Epoch: 263  ######### Train Loss: -4.600011618258762\n",
      "######### Epoch: 264  ######### Train Loss: -4.727276661126264\n",
      "######### Epoch: 265  ######### Train Loss: -4.7914035583226555\n",
      "######### Epoch: 266  ######### Train Loss: -4.793352491049434\n",
      "######### Epoch: 267  ######### Train Loss: -4.591737014524939\n",
      "######### Epoch: 268  ######### Train Loss: -4.676053372844677\n",
      "######### Epoch: 269  ######### Train Loss: -4.79689383067932\n",
      "######### Epoch: 270  ######### Train Loss: -4.971903806254589\n",
      "######### Epoch: 271  ######### Train Loss: -4.941514735604045\n",
      "######### Epoch: 272  ######### Train Loss: -4.661096341711019\n",
      "######### Epoch: 273  ######### Train Loss: -5.276609967928943\n",
      "######### Epoch: 274  ######### Train Loss: -4.0977607036605\n",
      "######### Epoch: 275  ######### Train Loss: -3.7648948192932212\n",
      "######### Epoch: 276  ######### Train Loss: -3.719590830190854\n",
      "######### Epoch: 277  ######### Train Loss: -3.848103552008662\n",
      "######### Epoch: 278  ######### Train Loss: -4.132594820536724\n",
      "######### Epoch: 279  ######### Train Loss: -4.263788954270636\n",
      "######### Epoch: 280  ######### Train Loss: -4.172070763090569\n",
      "######### Epoch: 281  ######### Train Loss: -4.347125359789997\n",
      "######### Epoch: 282  ######### Train Loss: -4.7916394290255235\n",
      "######### Epoch: 283  ######### Train Loss: -4.453237931996243\n",
      "######### Epoch: 284  ######### Train Loss: -4.532735529204228\n",
      "######### Epoch: 285  ######### Train Loss: -4.673232738118213\n",
      "######### Epoch: 286  ######### Train Loss: -5.029482642854206\n",
      "######### Epoch: 287  ######### Train Loss: -4.065314308145092\n",
      "######### Epoch: 288  ######### Train Loss: -3.677380733629825\n",
      "######### Epoch: 289  ######### Train Loss: -3.5757284563978433\n",
      "######### Epoch: 290  ######### Train Loss: -3.6202190706133996\n",
      "######### Epoch: 291  ######### Train Loss: -3.805183304889408\n",
      "######### Epoch: 292  ######### Train Loss: -4.200006519275617\n",
      "######### Epoch: 293  ######### Train Loss: -4.427208658235663\n",
      "######### Epoch: 294  ######### Train Loss: -4.177949752968209\n",
      "######### Epoch: 295  ######### Train Loss: -4.242145126711692\n",
      "######### Epoch: 296  ######### Train Loss: -4.679631071084802\n",
      "######### Epoch: 297  ######### Train Loss: -4.32464893333344\n",
      "######### Epoch: 298  ######### Train Loss: -4.130427947689518\n",
      "######### Epoch: 299  ######### Train Loss: -4.3381946424634465\n",
      "######### Epoch: 300  ######### Train Loss: -4.839894087856054\n",
      "######### Epoch: 301  ######### Train Loss: -4.839113571289607\n",
      "######### Epoch: 302  ######### Train Loss: -4.774845803404078\n",
      "######### Epoch: 303  ######### Train Loss: -4.84839682150257\n",
      "######### Epoch: 304  ######### Train Loss: -4.948716996152219\n",
      "######### Epoch: 305  ######### Train Loss: -5.057104920641301\n",
      "######### Epoch: 306  ######### Train Loss: -4.812391565023336\n",
      "######### Epoch: 307  ######### Train Loss: -4.8976313014705495\n",
      "######### Epoch: 308  ######### Train Loss: -4.9237053321033795\n",
      "######### Epoch: 309  ######### Train Loss: -4.973941195622347\n",
      "######### Epoch: 310  ######### Train Loss: -5.073931933383477\n",
      "######### Epoch: 311  ######### Train Loss: -4.6879412624151\n",
      "######### Epoch: 312  ######### Train Loss: -4.534931866527295\n",
      "######### Epoch: 313  ######### Train Loss: -4.948724803905784\n",
      "######### Epoch: 314  ######### Train Loss: -4.723396369355613\n",
      "######### Epoch: 315  ######### Train Loss: -4.5813411604589245\n",
      "######### Epoch: 316  ######### Train Loss: -4.684613634730205\n",
      "######### Epoch: 317  ######### Train Loss: -5.098583974119074\n",
      "######### Epoch: 318  ######### Train Loss: -4.373493142534827\n",
      "######### Epoch: 319  ######### Train Loss: -3.954112837880603\n",
      "######### Epoch: 320  ######### Train Loss: -3.842026860775327\n",
      "######### Epoch: 321  ######### Train Loss: -3.883565921594015\n",
      "######### Epoch: 322  ######### Train Loss: -4.076603238591073\n",
      "######### Epoch: 323  ######### Train Loss: -4.513433361949261\n",
      "######### Epoch: 324  ######### Train Loss: -4.749973326525073\n",
      "######### Epoch: 325  ######### Train Loss: -4.625628866630342\n",
      "######### Epoch: 326  ######### Train Loss: -4.528167530583781\n",
      "######### Epoch: 327  ######### Train Loss: -4.518696470411584\n",
      "######### Epoch: 328  ######### Train Loss: -4.936427006737833\n",
      "######### Epoch: 329  ######### Train Loss: -4.536756201872364\n",
      "######### Epoch: 330  ######### Train Loss: -4.161232945601204\n",
      "######### Epoch: 331  ######### Train Loss: -4.15067660294546\n",
      "######### Epoch: 332  ######### Train Loss: -4.416250291545139\n",
      "######### Epoch: 333  ######### Train Loss: -5.008984529947165\n",
      "######### Epoch: 334  ######### Train Loss: -4.718747107469752\n",
      "######### Epoch: 335  ######### Train Loss: -4.448290575264576\n",
      "######### Epoch: 336  ######### Train Loss: -4.433677895856881\n",
      "######### Epoch: 337  ######### Train Loss: -4.723932130806249\n",
      "######### Epoch: 338  ######### Train Loss: -5.152245902007498\n",
      "######### Epoch: 339  ######### Train Loss: -5.09407415944717\n",
      "######### Epoch: 340  ######### Train Loss: -4.977536662048001\n",
      "######### Epoch: 341  ######### Train Loss: -4.962759903811761\n",
      "######### Epoch: 342  ######### Train Loss: -5.3490109283855105\n",
      "######### Epoch: 343  ######### Train Loss: -4.3237680069222755\n",
      "######### Epoch: 344  ######### Train Loss: -3.920291280172781\n",
      "######### Epoch: 345  ######### Train Loss: -3.8006213220194973\n",
      "######### Epoch: 346  ######### Train Loss: -3.81745111014092\n",
      "######### Epoch: 347  ######### Train Loss: -3.949796935354338\n",
      "######### Epoch: 348  ######### Train Loss: -4.22076306579183\n",
      "######### Epoch: 349  ######### Train Loss: -4.549962418456235\n",
      "######### Epoch: 350  ######### Train Loss: -4.498419922391393\n",
      "######### Epoch: 351  ######### Train Loss: -4.4999661270137175\n",
      "######### Epoch: 352  ######### Train Loss: -4.683858455898569\n",
      "######### Epoch: 353  ######### Train Loss: -4.626916233491374\n",
      "######### Epoch: 354  ######### Train Loss: -4.660799382421768\n",
      "######### Epoch: 355  ######### Train Loss: -4.83106286009408\n",
      "######### Epoch: 356  ######### Train Loss: -4.752540006319837\n",
      "######### Epoch: 357  ######### Train Loss: -5.2911826728111295\n",
      "######### Epoch: 358  ######### Train Loss: -4.279107569082227\n",
      "######### Epoch: 359  ######### Train Loss: -3.885431064793593\n",
      "######### Epoch: 360  ######### Train Loss: -3.7667775150314693\n",
      "######### Epoch: 361  ######### Train Loss: -3.790152954084333\n",
      "######### Epoch: 362  ######### Train Loss: -3.9519396709440096\n",
      "######### Epoch: 363  ######### Train Loss: -4.35809201362052\n",
      "######### Epoch: 364  ######### Train Loss: -4.9895989754592005\n",
      "######### Epoch: 365  ######### Train Loss: -4.618872031675946\n",
      "######### Epoch: 366  ######### Train Loss: -4.753015675046388\n",
      "######### Epoch: 367  ######### Train Loss: -4.916078611276535\n",
      "######### Epoch: 368  ######### Train Loss: -4.935878017419965\n",
      "######### Epoch: 369  ######### Train Loss: -4.828217959558927\n",
      "######### Epoch: 370  ######### Train Loss: -4.905217546981214\n",
      "######### Epoch: 371  ######### Train Loss: -5.1020528179812965\n",
      "######### Epoch: 372  ######### Train Loss: -5.4140005738278285\n",
      "######### Epoch: 373  ######### Train Loss: -4.634829763443513\n",
      "######### Epoch: 374  ######### Train Loss: -4.3709259699321015\n",
      "######### Epoch: 375  ######### Train Loss: -4.418585285183953\n",
      "######### Epoch: 376  ######### Train Loss: -4.666426702142858\n",
      "######### Epoch: 377  ######### Train Loss: -5.004851758327763\n",
      "######### Epoch: 378  ######### Train Loss: -4.748367948082377\n",
      "######### Epoch: 379  ######### Train Loss: -4.5755467254368645\n",
      "######### Epoch: 380  ######### Train Loss: -4.8983142115999465\n",
      "######### Epoch: 381  ######### Train Loss: -4.846625807896227\n",
      "######### Epoch: 382  ######### Train Loss: -4.747014635557878\n",
      "######### Epoch: 383  ######### Train Loss: -5.020457878172599\n",
      "######### Epoch: 384  ######### Train Loss: -5.077208713208299\n",
      "######### Epoch: 385  ######### Train Loss: -4.858253808778723\n",
      "######### Epoch: 386  ######### Train Loss: -5.3510519842302795\n",
      "######### Epoch: 387  ######### Train Loss: -4.260104233646702\n",
      "######### Epoch: 388  ######### Train Loss: -3.7851332305020198\n",
      "######### Epoch: 389  ######### Train Loss: -3.5843549973343958\n",
      "######### Epoch: 390  ######### Train Loss: -3.4920364739862237\n",
      "######### Epoch: 391  ######### Train Loss: -3.463080846492461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### Epoch: 392  ######### Train Loss: -3.48083907804641\n",
      "######### Epoch: 393  ######### Train Loss: -3.5401052987331525\n",
      "######### Epoch: 394  ######### Train Loss: -3.642814145372925\n",
      "######### Epoch: 395  ######### Train Loss: -3.7974274453057215\n",
      "######### Epoch: 396  ######### Train Loss: -4.017030582254886\n",
      "######### Epoch: 397  ######### Train Loss: -4.290748622157637\n",
      "######### Epoch: 398  ######### Train Loss: -4.470052816952202\n",
      "######### Epoch: 399  ######### Train Loss: -4.466280548019511\n",
      "######### Epoch: 400  ######### Train Loss: -4.413014189898994\n",
      "######### Epoch: 401  ######### Train Loss: -4.399832804078619\n",
      "######### Epoch: 402  ######### Train Loss: -4.472256690151084\n",
      "######### Epoch: 403  ######### Train Loss: -4.706042376557632\n",
      "######### Epoch: 404  ######### Train Loss: -5.159241510369549\n",
      "######### Epoch: 405  ######### Train Loss: -4.755986281854339\n",
      "######### Epoch: 406  ######### Train Loss: -4.651658506753807\n",
      "######### Epoch: 407  ######### Train Loss: -4.954090016639966\n",
      "######### Epoch: 408  ######### Train Loss: -4.832196878997169\n",
      "######### Epoch: 409  ######### Train Loss: -4.766341620008425\n",
      "######### Epoch: 410  ######### Train Loss: -5.065555417684639\n",
      "######### Epoch: 411  ######### Train Loss: -5.094558892644283\n",
      "######### Epoch: 412  ######### Train Loss: -5.012077556445395\n",
      "######### Epoch: 413  ######### Train Loss: -4.924879933090773\n",
      "######### Epoch: 414  ######### Train Loss: -5.514000732717319\n",
      "######### Epoch: 415  ######### Train Loss: -4.57521562319784\n",
      "######### Epoch: 416  ######### Train Loss: -4.239520051280088\n",
      "######### Epoch: 417  ######### Train Loss: -4.191303593215574\n",
      "######### Epoch: 418  ######### Train Loss: -4.318341281253348\n",
      "######### Epoch: 419  ######### Train Loss: -4.603059382028611\n",
      "######### Epoch: 420  ######### Train Loss: -4.712037468572097\n",
      "######### Epoch: 421  ######### Train Loss: -4.672495171668412\n",
      "######### Epoch: 422  ######### Train Loss: -4.93728915142524\n",
      "######### Epoch: 423  ######### Train Loss: -4.9432977692924505\n",
      "######### Epoch: 424  ######### Train Loss: -4.873436625306428\n",
      "######### Epoch: 425  ######### Train Loss: -5.021124067996999\n",
      "######### Epoch: 426  ######### Train Loss: -4.915169434930836\n",
      "######### Epoch: 427  ######### Train Loss: -5.363793692783137\n",
      "######### Epoch: 428  ######### Train Loss: -4.586532605226482\n",
      "######### Epoch: 429  ######### Train Loss: -4.208933204079099\n",
      "######### Epoch: 430  ######### Train Loss: -4.13452393221401\n",
      "######### Epoch: 431  ######### Train Loss: -4.2429761751977\n",
      "######### Epoch: 432  ######### Train Loss: -4.5839817538933305\n",
      "######### Epoch: 433  ######### Train Loss: -4.9343483619409\n",
      "######### Epoch: 434  ######### Train Loss: -4.719421449020144\n",
      "######### Epoch: 435  ######### Train Loss: -4.928503492625088\n",
      "######### Epoch: 436  ######### Train Loss: -5.090632804354201\n",
      "######### Epoch: 437  ######### Train Loss: -5.006184762346525\n",
      "######### Epoch: 438  ######### Train Loss: -5.1399791220685165\n",
      "######### Epoch: 439  ######### Train Loss: -5.245867742110306\n",
      "######### Epoch: 440  ######### Train Loss: -5.061897969203115\n",
      "######### Epoch: 441  ######### Train Loss: -5.001939048669345\n",
      "######### Epoch: 442  ######### Train Loss: -5.336814518670463\n",
      "######### Epoch: 443  ######### Train Loss: -5.419941712380939\n",
      "######### Epoch: 444  ######### Train Loss: -5.326947357463904\n",
      "######### Epoch: 445  ######### Train Loss: -5.131889286008899\n",
      "######### Epoch: 446  ######### Train Loss: -5.463836235073314\n",
      "######### Epoch: 447  ######### Train Loss: -4.679917792447734\n",
      "######### Epoch: 448  ######### Train Loss: -4.335724727125758\n",
      "######### Epoch: 449  ######### Train Loss: -4.302291339860191\n",
      "######### Epoch: 450  ######### Train Loss: -4.461559535720988\n",
      "######### Epoch: 451  ######### Train Loss: -4.787674405103311\n",
      "######### Epoch: 452  ######### Train Loss: -4.8279323481659935\n",
      "######### Epoch: 453  ######### Train Loss: -4.815227791332489\n",
      "######### Epoch: 454  ######### Train Loss: -5.028472485739056\n",
      "######### Epoch: 455  ######### Train Loss: -4.972031353271832\n",
      "######### Epoch: 456  ######### Train Loss: -5.071653968034757\n",
      "######### Epoch: 457  ######### Train Loss: -4.9356734707420244\n",
      "######### Epoch: 458  ######### Train Loss: -5.01817084089625\n",
      "######### Epoch: 459  ######### Train Loss: -5.510346246369491\n",
      "######### Epoch: 460  ######### Train Loss: -5.867839818561391\n",
      "######### Epoch: 461  ######### Train Loss: -5.091136734156432\n",
      "######### Epoch: 462  ######### Train Loss: -4.976496980235309\n",
      "######### Epoch: 463  ######### Train Loss: -4.9942455089905495\n",
      "######### Epoch: 464  ######### Train Loss: -5.1531765239773915\n",
      "######### Epoch: 465  ######### Train Loss: -5.408881920206093\n",
      "######### Epoch: 466  ######### Train Loss: -5.158950563967254\n",
      "######### Epoch: 467  ######### Train Loss: -5.000813892846505\n",
      "######### Epoch: 468  ######### Train Loss: -5.209918936533671\n",
      "######### Epoch: 469  ######### Train Loss: -5.373848316444982\n",
      "######### Epoch: 470  ######### Train Loss: -5.338817444898296\n",
      "######### Epoch: 471  ######### Train Loss: -5.184993091873504\n",
      "######### Epoch: 472  ######### Train Loss: -5.316070986385897\n",
      "######### Epoch: 473  ######### Train Loss: -5.176426531828708\n",
      "######### Epoch: 474  ######### Train Loss: -5.236367089431185\n",
      "######### Epoch: 475  ######### Train Loss: -5.157184190974718\n",
      "######### Epoch: 476  ######### Train Loss: -5.395453361661507\n",
      "######### Epoch: 477  ######### Train Loss: -5.209185349491771\n",
      "######### Epoch: 478  ######### Train Loss: -5.335852873054251\n",
      "######### Epoch: 479  ######### Train Loss: -5.287761513178004\n",
      "######### Epoch: 480  ######### Train Loss: -5.291140843077901\n",
      "######### Epoch: 481  ######### Train Loss: -5.838341922061129\n",
      "######### Epoch: 482  ######### Train Loss: -4.799727542831369\n",
      "######### Epoch: 483  ######### Train Loss: -4.527949154172471\n",
      "######### Epoch: 484  ######### Train Loss: -4.546488519596835\n",
      "######### Epoch: 485  ######### Train Loss: -4.742003138309283\n",
      "######### Epoch: 486  ######### Train Loss: -4.994450860780793\n",
      "######### Epoch: 487  ######### Train Loss: -5.035614688458466\n",
      "######### Epoch: 488  ######### Train Loss: -4.955805771908296\n",
      "######### Epoch: 489  ######### Train Loss: -5.145841748661572\n",
      "######### Epoch: 490  ######### Train Loss: -5.374254844142023\n",
      "######### Epoch: 491  ######### Train Loss: -5.307925137319153\n",
      "######### Epoch: 492  ######### Train Loss: -5.542178521726457\n",
      "######### Epoch: 493  ######### Train Loss: -5.037834162372725\n",
      "######### Epoch: 494  ######### Train Loss: -5.14909250285298\n",
      "######### Epoch: 495  ######### Train Loss: -5.172717131439023\n",
      "######### Epoch: 496  ######### Train Loss: -5.215099673686226\n",
      "######### Epoch: 497  ######### Train Loss: -5.206216882683486\n",
      "######### Epoch: 498  ######### Train Loss: -5.365902641101005\n",
      "######### Epoch: 499  ######### Train Loss: -4.860270120569528\n",
      "######### Epoch: 500  ######### Train Loss: -4.597614600931262\n",
      "######### Epoch: 501  ######### Train Loss: -4.767924302893573\n",
      "######### Epoch: 502  ######### Train Loss: -5.593778371077871\n",
      "######### Epoch: 503  ######### Train Loss: -4.589717378778241\n",
      "######### Epoch: 504  ######### Train Loss: -4.142629967632751\n",
      "######### Epoch: 505  ######### Train Loss: -3.9557368482623785\n",
      "######### Epoch: 506  ######### Train Loss: -3.877020762245343\n",
      "######### Epoch: 507  ######### Train Loss: -3.865347201339032\n",
      "######### Epoch: 508  ######### Train Loss: -3.9081641060814376\n",
      "######### Epoch: 509  ######### Train Loss: -4.007057754382352\n",
      "######### Epoch: 510  ######### Train Loss: -4.177332606891645\n",
      "######### Epoch: 511  ######### Train Loss: -4.45305450046889\n",
      "######### Epoch: 512  ######### Train Loss: -4.820691087689216\n",
      "######### Epoch: 513  ######### Train Loss: -4.791612904102095\n",
      "######### Epoch: 514  ######### Train Loss: -4.605354405195146\n",
      "######### Epoch: 515  ######### Train Loss: -4.586880112828977\n",
      "######### Epoch: 516  ######### Train Loss: -4.737269122061441\n",
      "######### Epoch: 517  ######### Train Loss: -5.006613301510341\n",
      "######### Epoch: 518  ######### Train Loss: -5.1071837617814\n",
      "######### Epoch: 519  ######### Train Loss: -5.223278642099584\n",
      "######### Epoch: 520  ######### Train Loss: -4.995090908050551\n",
      "######### Epoch: 521  ######### Train Loss: -4.885072328676498\n",
      "######### Epoch: 522  ######### Train Loss: -5.31092025174435\n",
      "######### Epoch: 523  ######### Train Loss: -5.020900295811637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### Epoch: 524  ######### Train Loss: -4.6789793085656655\n",
      "######### Epoch: 525  ######### Train Loss: -4.751608394617475\n",
      "######### Epoch: 526  ######### Train Loss: -5.243485059512211\n",
      "######### Epoch: 527  ######### Train Loss: -5.04114640363794\n",
      "######### Epoch: 528  ######### Train Loss: -4.747415800031484\n",
      "######### Epoch: 529  ######### Train Loss: -4.813434614225443\n",
      "######### Epoch: 530  ######### Train Loss: -5.142478659002471\n",
      "######### Epoch: 531  ######### Train Loss: -5.252656089119221\n",
      "######### Epoch: 532  ######### Train Loss: -5.286051316008514\n",
      "######### Epoch: 533  ######### Train Loss: -5.102622808465208\n",
      "######### Epoch: 534  ######### Train Loss: -5.263815764335168\n",
      "######### Epoch: 535  ######### Train Loss: -5.44730610189624\n",
      "######### Epoch: 536  ######### Train Loss: -5.5329787887708095\n",
      "######### Epoch: 537  ######### Train Loss: -5.169406347814934\n",
      "######### Epoch: 538  ######### Train Loss: -4.977569707399635\n",
      "######### Epoch: 539  ######### Train Loss: -5.293762835528682\n",
      "######### Epoch: 540  ######### Train Loss: -5.305569390675831\n",
      "######### Epoch: 541  ######### Train Loss: -5.318542184578646\n",
      "######### Epoch: 542  ######### Train Loss: -5.267310385133776\n",
      "######### Epoch: 543  ######### Train Loss: -5.672105582502517\n",
      "######### Epoch: 544  ######### Train Loss: -4.764880905149794\n",
      "######### Epoch: 545  ######### Train Loss: -4.348966069062665\n",
      "######### Epoch: 546  ######### Train Loss: -4.221959079818048\n",
      "######### Epoch: 547  ######### Train Loss: -4.239131579938797\n",
      "######### Epoch: 548  ######### Train Loss: -4.3960918339151975\n",
      "######### Epoch: 549  ######### Train Loss: -4.815315418409279\n",
      "######### Epoch: 550  ######### Train Loss: -5.8612487524089305\n",
      "######### Epoch: 551  ######### Train Loss: -5.600922433608054\n",
      "######### Epoch: 552  ######### Train Loss: -5.363315657303209\n",
      "######### Epoch: 553  ######### Train Loss: -5.562343535784979\n",
      "######### Epoch: 554  ######### Train Loss: -5.3351848792961585\n",
      "######### Epoch: 555  ######### Train Loss: -5.587691263965002\n",
      "######### Epoch: 556  ######### Train Loss: -5.04364251310576\n",
      "######### Epoch: 557  ######### Train Loss: -4.892363617890464\n",
      "######### Epoch: 558  ######### Train Loss: -5.325654364634501\n",
      "######### Epoch: 559  ######### Train Loss: -5.066882962763921\n",
      "######### Epoch: 560  ######### Train Loss: -4.734400934380404\n",
      "######### Epoch: 561  ######### Train Loss: -4.823608035728419\n",
      "######### Epoch: 562  ######### Train Loss: -5.381527512199722\n",
      "######### Epoch: 563  ######### Train Loss: -4.967584405101345\n",
      "######### Epoch: 564  ######### Train Loss: -4.620253520597796\n",
      "######### Epoch: 565  ######### Train Loss: -4.595536430195582\n",
      "######### Epoch: 566  ######### Train Loss: -4.787199366298964\n",
      "######### Epoch: 567  ######### Train Loss: -5.172692222174452\n",
      "######### Epoch: 568  ######### Train Loss: -5.145046659513235\n",
      "######### Epoch: 569  ######### Train Loss: -5.119753146743725\n",
      "######### Epoch: 570  ######### Train Loss: -5.144721236337388\n",
      "######### Epoch: 571  ######### Train Loss: -5.149164887473359\n",
      "######### Epoch: 572  ######### Train Loss: -5.570444950049061\n",
      "######### Epoch: 573  ######### Train Loss: -4.954837683693536\n",
      "######### Epoch: 574  ######### Train Loss: -4.658477541117774\n",
      "######### Epoch: 575  ######### Train Loss: -4.720202846884222\n",
      "######### Epoch: 576  ######### Train Loss: -5.2119508296502115\n",
      "######### Epoch: 577  ######### Train Loss: -5.11933466760643\n",
      "######### Epoch: 578  ######### Train Loss: -4.761925252460644\n",
      "######### Epoch: 579  ######### Train Loss: -4.864125040017909\n",
      "######### Epoch: 580  ######### Train Loss: -5.656844684819572\n",
      "######### Epoch: 581  ######### Train Loss: -4.59223210613512\n",
      "######### Epoch: 582  ######### Train Loss: -4.082693948258742\n",
      "######### Epoch: 583  ######### Train Loss: -3.847194934968035\n",
      "######### Epoch: 584  ######### Train Loss: -3.717836049156655\n",
      "######### Epoch: 585  ######### Train Loss: -3.64764508870101\n",
      "######### Epoch: 586  ######### Train Loss: -3.6176809889843407\n",
      "######### Epoch: 587  ######### Train Loss: -3.6193907914525068\n",
      "######### Epoch: 588  ######### Train Loss: -3.649508208260122\n",
      "######### Epoch: 589  ######### Train Loss: -3.708576710211199\n",
      "######### Epoch: 590  ######### Train Loss: -3.8013056444634272\n",
      "######### Epoch: 591  ######### Train Loss: -3.939121964329104\n",
      "######### Epoch: 592  ######### Train Loss: -4.148126544864273\n",
      "######### Epoch: 593  ######### Train Loss: -4.496872012614595\n",
      "######### Epoch: 594  ######### Train Loss: -5.18023349403686\n",
      "######### Epoch: 595  ######### Train Loss: -4.889003787619824\n",
      "######### Epoch: 596  ######### Train Loss: -4.467568929508559\n",
      "######### Epoch: 597  ######### Train Loss: -4.309363991120346\n",
      "######### Epoch: 598  ######### Train Loss: -4.2719177833595365\n",
      "######### Epoch: 599  ######### Train Loss: -4.315305632770813\n",
      "######### Epoch: 600  ######### Train Loss: -4.430357952530101\n",
      "######### Epoch: 601  ######### Train Loss: -4.579153958775799\n",
      "######### Epoch: 602  ######### Train Loss: -4.813717453889256\n",
      "######### Epoch: 603  ######### Train Loss: -5.178797567217568\n",
      "######### Epoch: 604  ######### Train Loss: -5.496120190787695\n",
      "######### Epoch: 605  ######### Train Loss: -5.084351115135156\n",
      "######### Epoch: 606  ######### Train Loss: -4.8732123441241\n",
      "######### Epoch: 607  ######### Train Loss: -4.9784367089280215\n",
      "######### Epoch: 608  ######### Train Loss: -5.623538197636965\n",
      "######### Epoch: 609  ######### Train Loss: -4.9618167930523915\n",
      "######### Epoch: 610  ######### Train Loss: -4.501426654480971\n",
      "######### Epoch: 611  ######### Train Loss: -4.33511802286404\n",
      "######### Epoch: 612  ######### Train Loss: -4.294898962966487\n",
      "######### Epoch: 613  ######### Train Loss: -4.3447074847046245\n",
      "######### Epoch: 614  ######### Train Loss: -4.4898244705780295\n",
      "######### Epoch: 615  ######### Train Loss: -4.777431104898598\n",
      "######### Epoch: 616  ######### Train Loss: -5.278264076508771\n",
      "######### Epoch: 617  ######### Train Loss: -5.230786974929781\n",
      "######### Epoch: 618  ######### Train Loss: -5.0118874250826195\n",
      "######### Epoch: 619  ######### Train Loss: -4.947070259987716\n",
      "######### Epoch: 620  ######### Train Loss: -4.976602209950076\n",
      "######### Epoch: 621  ######### Train Loss: -5.154135361144824\n",
      "######### Epoch: 622  ######### Train Loss: -5.639372118073075\n",
      "######### Epoch: 623  ######### Train Loss: -5.1162740814952015\n",
      "######### Epoch: 624  ######### Train Loss: -4.8055370886549085\n",
      "######### Epoch: 625  ######### Train Loss: -4.824296996629926\n",
      "######### Epoch: 626  ######### Train Loss: -5.134902659601824\n",
      "######### Epoch: 627  ######### Train Loss: -5.5239224556494095\n",
      "######### Epoch: 628  ######### Train Loss: -5.33350119816402\n",
      "######### Epoch: 629  ######### Train Loss: -5.580519825575932\n",
      "######### Epoch: 630  ######### Train Loss: -5.432697143399163\n",
      "######### Epoch: 631  ######### Train Loss: -5.360625607754153\n",
      "######### Epoch: 632  ######### Train Loss: -5.4380706782829416\n",
      "######### Epoch: 633  ######### Train Loss: -5.766971914496181\n",
      "######### Epoch: 634  ######### Train Loss: -5.132109691465918\n",
      "######### Epoch: 635  ######### Train Loss: -4.754193858366273\n",
      "######### Epoch: 636  ######### Train Loss: -4.712658531274941\n",
      "######### Epoch: 637  ######### Train Loss: -4.892821529854517\n",
      "######### Epoch: 638  ######### Train Loss: -5.37607434402263\n",
      "######### Epoch: 639  ######### Train Loss: -5.25600132546343\n",
      "######### Epoch: 640  ######### Train Loss: -5.089961805793958\n",
      "######### Epoch: 641  ######### Train Loss: -5.285799841972819\n",
      "######### Epoch: 642  ######### Train Loss: -5.393318835747001\n",
      "######### Epoch: 643  ######### Train Loss: -5.332057355620876\n",
      "######### Epoch: 644  ######### Train Loss: -5.464974173344178\n",
      "######### Epoch: 645  ######### Train Loss: -5.515472127948064\n",
      "######### Epoch: 646  ######### Train Loss: -5.830344209362249\n",
      "######### Epoch: 647  ######### Train Loss: -5.450333705189432\n",
      "######### Epoch: 648  ######### Train Loss: -5.950059350666817\n",
      "######### Epoch: 649  ######### Train Loss: -4.834042156705017\n",
      "######### Epoch: 650  ######### Train Loss: -4.364018630146046\n",
      "######### Epoch: 651  ######### Train Loss: -4.165333638122441\n",
      "######### Epoch: 652  ######### Train Loss: -4.076956836453119\n",
      "######### Epoch: 653  ######### Train Loss: -4.05613570154351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### Epoch: 654  ######### Train Loss: -4.08952342554193\n",
      "######### Epoch: 655  ######### Train Loss: -4.178214690224338\n",
      "######### Epoch: 656  ######### Train Loss: -4.338391180342494\n",
      "######### Epoch: 657  ######### Train Loss: -4.616430242830481\n",
      "######### Epoch: 658  ######### Train Loss: -5.119150415432334\n",
      "######### Epoch: 659  ######### Train Loss: -5.151710199562376\n",
      "######### Epoch: 660  ######### Train Loss: -4.832560649051699\n",
      "######### Epoch: 661  ######### Train Loss: -4.7904846915481185\n",
      "######### Epoch: 662  ######### Train Loss: -4.99726183792661\n",
      "######### Epoch: 663  ######### Train Loss: -5.514932307107868\n",
      "######### Epoch: 664  ######### Train Loss: -5.102928776783854\n",
      "######### Epoch: 665  ######### Train Loss: -4.867251973888811\n",
      "######### Epoch: 666  ######### Train Loss: -4.956239695727897\n",
      "######### Epoch: 667  ######### Train Loss: -5.46374029485685\n",
      "######### Epoch: 668  ######### Train Loss: -5.227582970914029\n",
      "######### Epoch: 669  ######### Train Loss: -4.888982357521496\n",
      "######### Epoch: 670  ######### Train Loss: -4.901014508324665\n",
      "######### Epoch: 671  ######### Train Loss: -5.168696252270119\n",
      "######### Epoch: 672  ######### Train Loss: -5.4667376403837915\n",
      "######### Epoch: 673  ######### Train Loss: -5.47887944964577\n",
      "######### Epoch: 674  ######### Train Loss: -5.414633368923733\n",
      "######### Epoch: 675  ######### Train Loss: -5.317614448922095\n",
      "######### Epoch: 676  ######### Train Loss: -5.781962236046596\n",
      "######### Epoch: 677  ######### Train Loss: -5.081877915844992\n",
      "######### Epoch: 678  ######### Train Loss: -4.69558951104086\n",
      "######### Epoch: 679  ######### Train Loss: -4.617709748819233\n",
      "######### Epoch: 680  ######### Train Loss: -4.724273860414016\n",
      "######### Epoch: 681  ######### Train Loss: -5.08431628019174\n",
      "######### Epoch: 682  ######### Train Loss: -5.6397682750364195\n",
      "######### Epoch: 683  ######### Train Loss: -5.3515452576780165\n",
      "######### Epoch: 684  ######### Train Loss: -5.536273404775109\n",
      "######### Epoch: 685  ######### Train Loss: -5.43226289842736\n",
      "######### Epoch: 686  ######### Train Loss: -5.34170577520123\n",
      "######### Epoch: 687  ######### Train Loss: -5.567266832633841\n",
      "######### Epoch: 688  ######### Train Loss: -5.67068173171091\n",
      "######### Epoch: 689  ######### Train Loss: -5.598797896501531\n",
      "######### Epoch: 690  ######### Train Loss: -5.516685986372749\n",
      "######### Epoch: 691  ######### Train Loss: -5.99115192112942\n",
      "######### Epoch: 692  ######### Train Loss: -6.177070873762277\n",
      "######### Epoch: 693  ######### Train Loss: -5.620968480505409\n",
      "######### Epoch: 694  ######### Train Loss: -5.785082773978223\n",
      "######### Epoch: 695  ######### Train Loss: -5.747594596514334\n",
      "######### Epoch: 696  ######### Train Loss: -5.7038810459939215\n",
      "######### Epoch: 697  ######### Train Loss: -6.09678770634128\n",
      "######### Epoch: 698  ######### Train Loss: -6.3883306437300345\n",
      "######### Epoch: 699  ######### Train Loss: -6.2585783028889255\n",
      "######### Epoch: 700  ######### Train Loss: -5.016801726977186\n",
      "######### Epoch: 701  ######### Train Loss: -4.725607118026438\n",
      "######### Epoch: 702  ######### Train Loss: -4.6649517837906505\n",
      "######### Epoch: 703  ######### Train Loss: -4.755219786783522\n",
      "######### Epoch: 704  ######### Train Loss: -5.050201178628623\n",
      "######### Epoch: 705  ######### Train Loss: -5.954518575594026\n",
      "######### Epoch: 706  ######### Train Loss: -4.966829014572329\n",
      "######### Epoch: 707  ######### Train Loss: -4.46599581783734\n",
      "######### Epoch: 708  ######### Train Loss: -4.234721279538925\n",
      "######### Epoch: 709  ######### Train Loss: -4.108607380846661\n",
      "######### Epoch: 710  ######### Train Loss: -4.04105242344143\n",
      "######### Epoch: 711  ######### Train Loss: -4.013157592526762\n",
      "######### Epoch: 712  ######### Train Loss: -4.016288808912608\n",
      "######### Epoch: 713  ######### Train Loss: -4.046941510448388\n",
      "######### Epoch: 714  ######### Train Loss: -4.105097280001607\n",
      "######### Epoch: 715  ######### Train Loss: -4.194124380232501\n",
      "######### Epoch: 716  ######### Train Loss: -4.321968511776659\n",
      "######### Epoch: 717  ######### Train Loss: -4.504242168706052\n",
      "######### Epoch: 718  ######### Train Loss: -4.76956244016708\n",
      "######### Epoch: 719  ######### Train Loss: -5.144131817247368\n",
      "######### Epoch: 720  ######### Train Loss: -5.408186426242472\n",
      "######### Epoch: 721  ######### Train Loss: -5.343480005535638\n",
      "######### Epoch: 722  ######### Train Loss: -5.140596589752814\n",
      "######### Epoch: 723  ######### Train Loss: -5.002926222988783\n",
      "######### Epoch: 724  ######### Train Loss: -5.010712937868717\n",
      "######### Epoch: 725  ######### Train Loss: -5.226505409286497\n",
      "######### Epoch: 726  ######### Train Loss: -5.933016414988061\n",
      "######### Epoch: 727  ######### Train Loss: -5.145958359568287\n",
      "######### Epoch: 728  ######### Train Loss: -4.720378899868684\n",
      "######### Epoch: 729  ######### Train Loss: -4.566719324423181\n",
      "######### Epoch: 730  ######### Train Loss: -4.534764687418784\n",
      "######### Epoch: 731  ######### Train Loss: -4.590231629566011\n",
      "######### Epoch: 732  ######### Train Loss: -4.733301540509028\n",
      "######### Epoch: 733  ######### Train Loss: -4.978123273869681\n",
      "######### Epoch: 734  ######### Train Loss: -5.244320312052696\n",
      "######### Epoch: 735  ######### Train Loss: -5.2544612762862934\n",
      "######### Epoch: 736  ######### Train Loss: -5.274885278231443\n",
      "######### Epoch: 737  ######### Train Loss: -5.503341168489564\n",
      "######### Epoch: 738  ######### Train Loss: -5.520255370971831\n",
      "######### Epoch: 739  ######### Train Loss: -5.416128499714181\n",
      "######### Epoch: 740  ######### Train Loss: -5.46960988376987\n",
      "######### Epoch: 741  ######### Train Loss: -5.492075514614389\n",
      "######### Epoch: 742  ######### Train Loss: -5.807436389147109\n",
      "######### Epoch: 743  ######### Train Loss: -5.552479269145901\n",
      "######### Epoch: 744  ######### Train Loss: -5.228918398158632\n",
      "######### Epoch: 745  ######### Train Loss: -5.349290214971758\n",
      "######### Epoch: 746  ######### Train Loss: -5.79671055915981\n",
      "######### Epoch: 747  ######### Train Loss: -5.6712914966758206\n",
      "######### Epoch: 748  ######### Train Loss: -5.68691062942462\n",
      "######### Epoch: 749  ######### Train Loss: -5.7015120140548134\n",
      "######### Epoch: 750  ######### Train Loss: -5.747975013252487\n",
      "######### Epoch: 751  ######### Train Loss: -5.735944916083151\n",
      "######### Epoch: 752  ######### Train Loss: -6.347431350966376\n",
      "######### Epoch: 753  ######### Train Loss: -5.209052739838541\n",
      "######### Epoch: 754  ######### Train Loss: -4.884028116149661\n",
      "######### Epoch: 755  ######### Train Loss: -4.8499805220681935\n",
      "######### Epoch: 756  ######### Train Loss: -5.0236085990808075\n",
      "######### Epoch: 757  ######### Train Loss: -5.538252112244093\n",
      "######### Epoch: 758  ######### Train Loss: -5.424336231767384\n",
      "######### Epoch: 759  ######### Train Loss: -5.134451678947186\n",
      "######### Epoch: 760  ######### Train Loss: -5.22938410238223\n",
      "######### Epoch: 761  ######### Train Loss: -5.746302268504477\n",
      "######### Epoch: 762  ######### Train Loss: -5.339112802043016\n",
      "######### Epoch: 763  ######### Train Loss: -5.025362908029428\n",
      "######### Epoch: 764  ######### Train Loss: -5.031197679761537\n",
      "######### Epoch: 765  ######### Train Loss: -5.30475206004738\n",
      "######### Epoch: 766  ######### Train Loss: -5.8010513758504185\n",
      "######### Epoch: 767  ######### Train Loss: -5.586537349461756\n",
      "######### Epoch: 768  ######### Train Loss: -5.467838549365474\n",
      "######### Epoch: 769  ######### Train Loss: -5.480744296347124\n",
      "######### Epoch: 770  ######### Train Loss: -5.790607905330858\n",
      "######### Epoch: 771  ######### Train Loss: -5.631343652002496\n",
      "######### Epoch: 772  ######### Train Loss: -5.349895443386766\n",
      "######### Epoch: 773  ######### Train Loss: -5.642887026729334\n",
      "######### Epoch: 774  ######### Train Loss: -6.0134643498948375\n",
      "######### Epoch: 775  ######### Train Loss: -5.819213181590326\n",
      "######### Epoch: 776  ######### Train Loss: -6.103086584009733\n",
      "######### Epoch: 777  ######### Train Loss: -5.320324467162932\n",
      "######### Epoch: 778  ######### Train Loss: -4.9014893224379925\n",
      "######### Epoch: 779  ######### Train Loss: -4.78955589240428\n",
      "######### Epoch: 780  ######### Train Loss: -4.827988083723772\n",
      "######### Epoch: 781  ######### Train Loss: -5.009697997587506\n",
      "######### Epoch: 782  ######### Train Loss: -5.40746146992988\n",
      "######### Epoch: 783  ######### Train Loss: -5.768621432960655\n",
      "######### Epoch: 784  ######### Train Loss: -5.603517737864308\n",
      "######### Epoch: 785  ######### Train Loss: -5.350250037961512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### Epoch: 786  ######### Train Loss: -5.407598152236391\n",
      "######### Epoch: 787  ######### Train Loss: -6.06587079116066\n",
      "######### Epoch: 788  ######### Train Loss: -5.153783045072465\n",
      "######### Epoch: 789  ######### Train Loss: -4.679497117807573\n",
      "######### Epoch: 790  ######### Train Loss: -4.4841622550139\n",
      "######### Epoch: 791  ######### Train Loss: -4.40123057637067\n",
      "######### Epoch: 792  ######### Train Loss: -4.38700049712042\n",
      "######### Epoch: 793  ######### Train Loss: -4.427934526238812\n",
      "######### Epoch: 794  ######### Train Loss: -4.524728061608639\n",
      "######### Epoch: 795  ######### Train Loss: -4.690789926102016\n",
      "######### Epoch: 796  ######### Train Loss: -4.955009866068098\n",
      "######### Epoch: 797  ######### Train Loss: -5.316319829635918\n",
      "######### Epoch: 798  ######### Train Loss: -5.458135066369055\n",
      "######### Epoch: 799  ######### Train Loss: -5.404305456747854\n",
      "######### Epoch: 800  ######### Train Loss: -5.316813349778291\n",
      "######### Epoch: 801  ######### Train Loss: -5.302071274572885\n",
      "######### Epoch: 802  ######### Train Loss: -5.448662203918413\n",
      "######### Epoch: 803  ######### Train Loss: -5.860653617298105\n",
      "######### Epoch: 804  ######### Train Loss: -5.633374240929659\n",
      "######### Epoch: 805  ######### Train Loss: -5.410010837143896\n",
      "######### Epoch: 806  ######### Train Loss: -5.577064010022139\n",
      "######### Epoch: 807  ######### Train Loss: -6.013718612392449\n",
      "######### Epoch: 808  ######### Train Loss: -5.773549139598729\n",
      "######### Epoch: 809  ######### Train Loss: -5.809696248531771\n",
      "######### Epoch: 810  ######### Train Loss: -5.985204568375453\n",
      "######### Epoch: 811  ######### Train Loss: -5.9426137698443515\n",
      "######### Epoch: 812  ######### Train Loss: -5.905789565068976\n",
      "######### Epoch: 813  ######### Train Loss: -6.535902816291293\n",
      "######### Epoch: 814  ######### Train Loss: -6.07035276055555\n",
      "######### Epoch: 815  ######### Train Loss: -6.275163853801274\n",
      "######### Epoch: 816  ######### Train Loss: -6.113943732579284\n",
      "######### Epoch: 817  ######### Train Loss: -6.662078986801564\n",
      "######### Epoch: 818  ######### Train Loss: -5.813972578834871\n",
      "######### Epoch: 819  ######### Train Loss: -5.77348406387594\n",
      "######### Epoch: 820  ######### Train Loss: -6.306773773965904\n",
      "######### Epoch: 821  ######### Train Loss: -5.7145206435766545\n",
      "######### Epoch: 822  ######### Train Loss: -5.395804394409394\n",
      "######### Epoch: 823  ######### Train Loss: -5.417573480358395\n",
      "######### Epoch: 824  ######### Train Loss: -5.7121949858352155\n",
      "######### Epoch: 825  ######### Train Loss: -6.093157052677413\n",
      "######### Epoch: 826  ######### Train Loss: -5.914036489517763\n",
      "######### Epoch: 827  ######### Train Loss: -6.036543340190518\n",
      "######### Epoch: 828  ######### Train Loss: -5.911235498832204\n",
      "######### Epoch: 829  ######### Train Loss: -5.818607788430598\n",
      "######### Epoch: 830  ######### Train Loss: -6.360962192736116\n",
      "######### Epoch: 831  ######### Train Loss: -5.630838447168151\n",
      "######### Epoch: 832  ######### Train Loss: -5.206826656942737\n",
      "######### Epoch: 833  ######### Train Loss: -5.080711807641988\n",
      "######### Epoch: 834  ######### Train Loss: -5.097774338019988\n",
      "######### Epoch: 835  ######### Train Loss: -5.2435630380927245\n",
      "######### Epoch: 836  ######### Train Loss: -5.576746188428038\n",
      "######### Epoch: 837  ######### Train Loss: -6.030162062128663\n",
      "######### Epoch: 838  ######### Train Loss: -5.812069357716166\n",
      "######### Epoch: 839  ######### Train Loss: -5.806157342969455\n",
      "######### Epoch: 840  ######### Train Loss: -6.0122301031546685\n",
      "######### Epoch: 841  ######### Train Loss: -5.794622370805756\n",
      "######### Epoch: 842  ######### Train Loss: -5.815606633319668\n",
      "######### Epoch: 843  ######### Train Loss: -6.420395684937892\n",
      "######### Epoch: 844  ######### Train Loss: -5.614565814861357\n",
      "######### Epoch: 845  ######### Train Loss: -5.212825142423965\n",
      "######### Epoch: 846  ######### Train Loss: -5.088716484453882\n",
      "######### Epoch: 847  ######### Train Loss: -5.095366006804452\n",
      "######### Epoch: 848  ######### Train Loss: -5.206556632932639\n",
      "######### Epoch: 849  ######### Train Loss: -5.4321323280477944\n",
      "######### Epoch: 850  ######### Train Loss: -5.7249035494947265\n",
      "######### Epoch: 851  ######### Train Loss: -5.7577351299997135\n",
      "######### Epoch: 852  ######### Train Loss: -5.788507095242502\n",
      "######### Epoch: 853  ######### Train Loss: -6.184025763378663\n",
      "######### Epoch: 854  ######### Train Loss: -5.716463607691296\n",
      "######### Epoch: 855  ######### Train Loss: -5.412129142129907\n",
      "######### Epoch: 856  ######### Train Loss: -5.416895690181534\n",
      "######### Epoch: 857  ######### Train Loss: -5.702722535802872\n",
      "######### Epoch: 858  ######### Train Loss: -6.524855165471917\n",
      "######### Epoch: 859  ######### Train Loss: -5.471547308167455\n",
      "######### Epoch: 860  ######### Train Loss: -5.107961522154562\n",
      "######### Epoch: 861  ######### Train Loss: -5.0089637730157435\n",
      "######### Epoch: 862  ######### Train Loss: -5.034420067311893\n",
      "######### Epoch: 863  ######### Train Loss: -5.137963004626556\n",
      "######### Epoch: 864  ######### Train Loss: -5.257919960304104\n",
      "######### Epoch: 865  ######### Train Loss: -5.31192697029037\n",
      "######### Epoch: 866  ######### Train Loss: -5.345872639122586\n",
      "######### Epoch: 867  ######### Train Loss: -5.4736542005949484\n",
      "######### Epoch: 868  ######### Train Loss: -5.732353089007642\n",
      "######### Epoch: 869  ######### Train Loss: -5.95859759879754\n",
      "######### Epoch: 870  ######### Train Loss: -6.147173048966531\n",
      "######### Epoch: 871  ######### Train Loss: -5.8019139529906925\n",
      "######### Epoch: 872  ######### Train Loss: -5.994092851140154\n",
      "######### Epoch: 873  ######### Train Loss: -5.941646423873542\n",
      "######### Epoch: 874  ######### Train Loss: -6.112012708525923\n",
      "######### Epoch: 875  ######### Train Loss: -5.839095379666338\n",
      "######### Epoch: 876  ######### Train Loss: -5.719948723218778\n",
      "######### Epoch: 877  ######### Train Loss: -5.926537709403665\n",
      "######### Epoch: 878  ######### Train Loss: -5.875682117674046\n",
      "######### Epoch: 879  ######### Train Loss: -6.228540805847673\n",
      "######### Epoch: 880  ######### Train Loss: -5.883697093207537\n",
      "######### Epoch: 881  ######### Train Loss: -5.962944826559721\n",
      "######### Epoch: 882  ######### Train Loss: -5.785397583937681\n",
      "######### Epoch: 883  ######### Train Loss: -5.761444966546119\n",
      "######### Epoch: 884  ######### Train Loss: -6.320548001332708\n",
      "######### Epoch: 885  ######### Train Loss: -5.920340742329518\n",
      "######### Epoch: 886  ######### Train Loss: -6.091559143055796\n",
      "######### Epoch: 887  ######### Train Loss: -5.774638130215385\n",
      "######### Epoch: 888  ######### Train Loss: -5.786797073480469\n",
      "######### Epoch: 889  ######### Train Loss: -6.12779974463769\n",
      "######### Epoch: 890  ######### Train Loss: -6.2426809759852775\n",
      "######### Epoch: 891  ######### Train Loss: -5.712298504865057\n",
      "######### Epoch: 892  ######### Train Loss: -5.452857049744722\n",
      "######### Epoch: 893  ######### Train Loss: -5.640547609806422\n",
      "######### Epoch: 894  ######### Train Loss: -6.01547971065354\n",
      "######### Epoch: 895  ######### Train Loss: -5.802535182588324\n",
      "######### Epoch: 896  ######### Train Loss: -6.333673089633073\n",
      "######### Epoch: 897  ######### Train Loss: -5.362713153123861\n",
      "######### Epoch: 898  ######### Train Loss: -4.894394110654861\n",
      "######### Epoch: 899  ######### Train Loss: -4.705928706596012\n",
      "######### Epoch: 900  ######### Train Loss: -4.631157241951212\n",
      "######### Epoch: 901  ######### Train Loss: -4.628665414737326\n",
      "######### Epoch: 902  ######### Train Loss: -4.6711943098009066\n",
      "######### Epoch: 903  ######### Train Loss: -4.758960333690212\n",
      "######### Epoch: 904  ######### Train Loss: -4.898271700410284\n",
      "######### Epoch: 905  ######### Train Loss: -5.0956286130372686\n",
      "######### Epoch: 906  ######### Train Loss: -5.316310543289258\n",
      "######### Epoch: 907  ######### Train Loss: -5.398482099985566\n",
      "######### Epoch: 908  ######### Train Loss: -5.344962290580429\n",
      "######### Epoch: 909  ######### Train Loss: -5.353283071747265\n",
      "######### Epoch: 910  ######### Train Loss: -5.510545774534401\n",
      "######### Epoch: 911  ######### Train Loss: -5.85855731012685\n",
      "######### Epoch: 912  ######### Train Loss: -5.93309358088702\n",
      "######### Epoch: 913  ######### Train Loss: -5.93985973082921\n",
      "######### Epoch: 914  ######### Train Loss: -5.905835896838773\n",
      "######### Epoch: 915  ######### Train Loss: -5.705234689603368\n",
      "######### Epoch: 916  ######### Train Loss: -5.828901144546123\n",
      "######### Epoch: 917  ######### Train Loss: -6.382523508851972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### Epoch: 918  ######### Train Loss: -6.000002779638428\n",
      "######### Epoch: 919  ######### Train Loss: -6.31096961736612\n",
      "######### Epoch: 920  ######### Train Loss: -5.679088316864967\n",
      "######### Epoch: 921  ######### Train Loss: -5.322101023693806\n",
      "######### Epoch: 922  ######### Train Loss: -5.2834347166882525\n",
      "######### Epoch: 923  ######### Train Loss: -5.469996776504093\n",
      "######### Epoch: 924  ######### Train Loss: -6.041208892727939\n",
      "######### Epoch: 925  ######### Train Loss: -5.733547078321426\n",
      "######### Epoch: 926  ######### Train Loss: -5.407192492135444\n",
      "######### Epoch: 927  ######### Train Loss: -5.412844680011158\n",
      "######### Epoch: 928  ######### Train Loss: -5.706855894971564\n",
      "######### Epoch: 929  ######### Train Loss: -6.199803702723116\n",
      "######### Epoch: 930  ######### Train Loss: -5.906782757706313\n",
      "######### Epoch: 931  ######### Train Loss: -5.920259170695008\n",
      "######### Epoch: 932  ######### Train Loss: -6.039622161837793\n",
      "######### Epoch: 933  ######### Train Loss: -6.304048037389718\n",
      "######### Epoch: 934  ######### Train Loss: -5.83353301765891\n",
      "######### Epoch: 935  ######### Train Loss: -5.511718805497391\n",
      "######### Epoch: 936  ######### Train Loss: -5.58716609352009\n",
      "######### Epoch: 937  ######### Train Loss: -6.136992208310189\n",
      "######### Epoch: 938  ######### Train Loss: -5.820833796746971\n",
      "######### Epoch: 939  ######### Train Loss: -5.459199343383662\n",
      "######### Epoch: 940  ######### Train Loss: -5.438883811751065\n",
      "######### Epoch: 941  ######### Train Loss: -5.639141139026975\n",
      "######### Epoch: 942  ######### Train Loss: -6.010517368237345\n",
      "######### Epoch: 943  ######### Train Loss: -6.064657382750371\n",
      "######### Epoch: 944  ######### Train Loss: -5.952696303237152\n",
      "######### Epoch: 945  ######### Train Loss: -5.815684230711006\n",
      "######### Epoch: 946  ######### Train Loss: -6.060924667429785\n",
      "######### Epoch: 947  ######### Train Loss: -6.393166877815396\n",
      "######### Epoch: 948  ######### Train Loss: -6.510263064353481\n",
      "######### Epoch: 949  ######### Train Loss: -5.935365186913383\n",
      "######### Epoch: 950  ######### Train Loss: -5.678106801693207\n",
      "######### Epoch: 951  ######### Train Loss: -5.797058572700233\n",
      "######### Epoch: 952  ######### Train Loss: -6.178696089204325\n",
      "######### Epoch: 953  ######### Train Loss: -6.257687160826411\n",
      "######### Epoch: 954  ######### Train Loss: -5.9860859365539545\n",
      "######### Epoch: 955  ######### Train Loss: -6.166787021951728\n",
      "######### Epoch: 956  ######### Train Loss: -6.389684056177801\n",
      "######### Epoch: 957  ######### Train Loss: -6.504861032682789\n",
      "######### Epoch: 958  ######### Train Loss: -6.188514518636691\n",
      "######### Epoch: 959  ######### Train Loss: -6.092576120149849\n",
      "######### Epoch: 960  ######### Train Loss: -6.429315347146589\n",
      "######### Epoch: 961  ######### Train Loss: -6.0114194025700485\n",
      "######### Epoch: 962  ######### Train Loss: -5.7090888638059605\n",
      "######### Epoch: 963  ######### Train Loss: -5.83383374492002\n",
      "######### Epoch: 964  ######### Train Loss: -6.425546953173579\n",
      "######### Epoch: 965  ######### Train Loss: -5.97126293311243\n",
      "######### Epoch: 966  ######### Train Loss: -5.5540382358396965\n",
      "######### Epoch: 967  ######### Train Loss: -5.462232250270616\n",
      "######### Epoch: 968  ######### Train Loss: -5.560728273803782\n",
      "######### Epoch: 969  ######### Train Loss: -5.933365111791629\n",
      "######### Epoch: 970  ######### Train Loss: -6.730823655982762\n",
      "######### Epoch: 971  ######### Train Loss: -6.286371289438582\n",
      "######### Epoch: 972  ######### Train Loss: -6.264444370524474\n",
      "######### Epoch: 973  ######### Train Loss: -6.3236046152351335\n",
      "######### Epoch: 974  ######### Train Loss: -6.771161831715054\n",
      "######### Epoch: 975  ######### Train Loss: -5.873216580406231\n",
      "######### Epoch: 976  ######### Train Loss: -5.522384653591582\n",
      "######### Epoch: 977  ######### Train Loss: -5.461296065910997\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "# epochs = 250\n",
    "epochs = 4000\n",
    "step_size = 100\n",
    "gamma = 0.75\n",
    "\n",
    "modes = 18\n",
    "width = 256\n",
    "\n",
    "# model\n",
    "\n",
    "fno = FNO1d(modes, width).to(device)\n",
    "\n",
    "optimizer = Adam(fno.parameters(), lr=learning_rate, weight_decay=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "\n",
    "l = torch.nn.MSELoss()\n",
    "freq_print = 1\n",
    "for epoch in range(epochs):\n",
    "    train_mse = 0.0\n",
    "    for step, (input_batch, output_batch) in enumerate(training_set):\n",
    "        optimizer.zero_grad()\n",
    "        output_pred_batch = fno(input_batch)\n",
    "        output_pred_batch = fno(input_batch).squeeze(0)\n",
    "        output_batch = output_batch.squeeze(0)\n",
    "        loss_f = torch.log10(l(output_pred_batch, output_batch))\n",
    "        loss_f.backward()\n",
    "        optimizer.step()\n",
    "        train_mse += loss_f.item()\n",
    "    train_mse /= len(training_set)\n",
    "\n",
    "    scheduler.step()\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        fno.eval()\n",
    "        test_relative_l2 = 0.0\n",
    "        for step, (input_batch, output_batch) in enumerate(testing_set):\n",
    "            output_pred_batch = fno(input_batch).squeeze(2)\n",
    "            loss_f = torch.log10(l(output_pred_batch, output_batch))\n",
    "            test_relative_l2 += loss_f.item()\n",
    "        test_relative_l2 /= len(testing_set)\n",
    "     \"\"\"\n",
    "\n",
    "    if epoch % freq_print == 0: print(\"######### Epoch:\", epoch, \" ######### Train Loss:\", train_mse,)#\" ######### Relative L2 Test Norm:\", test_relative_l2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mohWn-pNqAK",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_read = pd.read_csv(r'C:\\Users\\matth\\OneDrive\\Documents\\TestingData.txt')\n",
    "test_read = test_read.iloc[:,0:1].values\n",
    "test_read = torch.tensor(test_read).to(torch.float64).squeeze(1).to(device)\n",
    "test_read /= 602168.58\n",
    "\n",
    "output_function_test_0 = fno(input_function_train[0,:,:].unsqueeze(0))\n",
    "output_function_test_1 = fno(input_function_train[35,:,:].unsqueeze(0))\n",
    "output_function_test_2 = fno(input_function_train[70,:,:].unsqueeze(0))\n",
    "output_function_test_3 = fno(input_function_train[105,:,:].unsqueeze(0))\n",
    "output_function_test_4 = fno(input_function_train[140,:,:].unsqueeze(0))\n",
    "#output_function_test_4 = fno(data_read[:,140:175].to(device))\n",
    "\n",
    "\n",
    "next_step = torch.cat((output_function_train[140,:,:].unsqueeze(0), data_read[:,175:210, 2].unsqueeze(2)), dim=2)\n",
    "print(next_step.shape)\n",
    "output_function_test_5 = fno(next_step)\n",
    "\n",
    "interval = torch.linspace(520392.6,693856.8, 70).to(device)\n",
    "interval /=  602168.58\n",
    "next_step = torch.cat((output_function_test_5, interval[:35].unsqueeze(0).unsqueeze(2)), dim=2)\n",
    "output_function_test_6 = fno(next_step)\n",
    "\n",
    "input_function_pred = torch.cat((input_function_train[35,:,2], input_function_train[70,:,2], input_function_train[105,:,2], data_read[:,140:210,2].squeeze(0),interval), dim=0) #data_read[:,175:210,2].squeeze(0).to(device), test_read), dim=0)\n",
    "output_function_pred = torch.cat((output_function_test_0, output_function_test_1, output_function_test_2, output_function_test_3, output_function_test_4, output_function_test_5, output_function_test_6), dim=1)\n",
    "\n",
    "\n",
    "input_function_plot = torch.cat((input_function_train[0,:,2], input_function_train[35,:,2], input_function_train[70,:,2], input_function_train[105,:,2], data_read[:,140:210,2].squeeze(0).to(device)), dim=0)\n",
    "output_function_plot = torch.cat((input_function_train[0,:,0], input_function_train[35,:,0], input_function_train[70,:,0], input_function_train[105,:,0], data_read[:,140:210,0].squeeze(0).to(device)), dim=0)\n",
    "\n",
    "diff = abs(output_function_pred[:,:175, 0] - output_function_plot[35:]).squeeze(0)\n",
    "\n",
    "plt.figure(dpi=1500)\n",
    "plt.grid(True, which=\"both\", ls=\":\")\n",
    "\n",
    "plt.plot(input_function_plot.cpu().detach(), output_function_plot.cpu().detach(), label=\"True Solution\", c=\"C0\", lw=2)\n",
    "plt.plot(input_function_pred.squeeze(0).cpu().detach(), output_function_pred[:,:245,0].squeeze(0).cpu().detach(), label=\"Approximate Solution\", c=\"C1\", lw=1)\n",
    "\n",
    "#plt.plot(input_function_plot[35:].cpu().detach(), diff.cpu().detach(), label=\"True Solution\", c=\"C0\", lw=2)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Thh7zoLwTcFb"
   },
   "outputs": [],
   "source": [
    "test_read = pd.read_csv(r'C:\\Users\\matth\\OneDrive\\Documents\\TestingData.txt')\n",
    "test_read = test_read.iloc[:,0:1].values\n",
    "test_read = torch.tensor(test_read).to(torch.float64).squeeze(1).to(device)\n",
    "test_read /= 602168.58\n",
    "\n",
    "saved_pred = pd.read_csv(r'C:\\Users\\matth\\OneDrive\\Documents\\tensor_data.txt')\n",
    "saved_pred = saved_pred.iloc[:,1:3].values\n",
    "saved_pred = torch.tensor(saved_pred).to(torch.float64).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "output_function_test_0 = fno(input_function_train[0,:,:].unsqueeze(0))\n",
    "output_function_test_1 = fno(input_function_train[1,:,:].unsqueeze(0))\n",
    "output_function_test_2 = fno(input_function_train[2,:,:].unsqueeze(0))\n",
    "output_function_test_3 = fno(input_function_train[3,:,:].unsqueeze(0))\n",
    "output_function_test_4 = fno(input_function_train[4,:,:].unsqueeze(0))\n",
    "output_function_test_5 = fno(data_read[:,175:210].to(device))\n",
    "\n",
    "saved_pred[:,:,0] /= max_Tf\n",
    "saved_pred[:,:,1] /= max_Ts\n",
    "print(saved_pred.shape)\n",
    "input_function_pred = 602168.58 * torch.cat((input_function_train[1,:,2], input_function_train[2,:,2], input_function_train[3,:,2], input_function_train[4,:,2], data_read[:,175:210,2].squeeze(0).to(device), test_read), dim=0)\n",
    "output_function_pred = torch.cat((output_function_test_0, output_function_test_1, output_function_test_2, output_function_test_3, output_function_test_4, saved_pred), dim=1)\n",
    "print(input_function_pred.shape, output_function_pred.shape)\n",
    "\n",
    "input_function_plot = 602168.58 * torch.cat((input_function_train[0,:,2], input_function_train[1,:,2], input_function_train[2,:,2], input_function_train[3,:,2], input_function_train[4,:,2], data_read[:,175:210,2].squeeze(0).to(device)), dim=0)\n",
    "output_function_plot = torch.cat((input_function_train[0,:,0:2], input_function_train[1,:,0:2], input_function_train[2,:,0:2], input_function_train[3,:,0:2], input_function_train[4,:,0:2], output_function_train[4,:,0:2]), dim=0)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(dpi=1000)\n",
    "plt.grid(True, which=\"both\", ls=\":\")\n",
    "print(output_function_pred.shape)\n",
    "plt.plot(input_function_pred.cpu().detach(), max_Tf * output_function_pred[:,:,0].squeeze(0).cpu().detach(), label=\"Approximate Solution\", c=\"C1\", lw=0.5)\n",
    "plt.plot(input_function_plot.cpu().detach(), max_Tf * output_function_plot[:,0].cpu().detach(), label=\"True Solution\", c=\"C0\", lw=0.5)\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(dpi=250)\n",
    "plt.grid(True, which=\"both\", ls=\":\")\n",
    "\n",
    "plt.plot(input_function_pred.cpu().detach(), max_Ts * output_function_pred[:,:,1].squeeze(0).cpu().detach(), label=\"Approximate Solution\", c=\"C1\", lw=2)\n",
    "plt.plot(input_function_plot.cpu().detach(), max_Ts * output_function_plot[:,1].cpu().detach(), label=\"True Solution\", c=\"C0\", lw=2)\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cGglcV3VTcFd"
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame from the numpy array\n",
    "dt = pd.read_csv(r'C:\\Users\\matth\\OneDrive\\Documents\\TestingData.txt')\n",
    "test_read = dt.iloc[:,0:1].values\n",
    "test_read = torch.tensor(test_read, dtype = torch.float64).squeeze(1).to(device)\n",
    "submission = torch.empty(34,3)\n",
    "submission[:, 0] = test_read\n",
    "submission[:, 1:3] = output_function_test_5[0,:34, 0:2]\n",
    "submission[:, 1] *= max_Tf\n",
    "submission[:, 2] *= max_Ts\n",
    "\n",
    "\n",
    "submission = pd.DataFrame(submission.detach().numpy())\n",
    "\n",
    "# Save the DataFrame as a text file\n",
    "submission.to_csv(r'C:\\Users\\matth\\OneDrive\\Documents\\tensor_data.txt', sep=',', index=False, header=['t', 'tf0', 'ts0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LhMssUKlTcFe"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "data_tens = pd.read_csv(r'C:\\Users\\matth\\OneDrive\\Documents\\tensor_data.txt')\n",
    "print(data_tens.shape)\n",
    "subp = pd.read_csv(r'C:\\Users\\matth\\Downloads\\submissiontask3.txt')\n",
    "y_1 = data_tens.iloc[:,0:3].values\n",
    "y_2 = subp.iloc[:,0:3].values\n",
    "\n",
    "y_1 = torch.tensor(y_1).to(torch.float64).squeeze(1)\n",
    "y_2 = torch.tensor(y_2).to(torch.float64).squeeze(1)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(dpi=250)\n",
    "plt.grid(True, which=\"both\", ls=\":\")\n",
    "\n",
    "plt.plot(y_1[:,0].cpu().detach(), y_1[:,1].cpu().detach(), label=\"Matthias Solution\", c=\"C1\", lw=1)\n",
    "plt.plot(y_2[:,0].cpu().detach(), y_2[:,1].cpu().detach(), label=\"Paul Solution\", c=\"C0\", lw=1)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
